[{"content":"1. 단일 Deicision Tree의 한계 저번 글에서 Decision Tree에 대해서 자세히 알아보았다. Decision Tree는 직관적이고 사람이 해석 가능하며 매우 좋은 알고리즘인 건 분명하다. 하지만 몇 개의 문제가 있다.\n단일 Decision Tree의 가장 큰 문제 - Overfitting 데이터가 매우 복잡하고 해석하기 어려울 때 Deicision Tree에서 분할은 어떻게 될까? 데이터의 feature가 1개, 2개, 10개 이하 이렇게 feature의 수가 적다면 데이터를 분할하기 쉽고 빠르게 데이터를 분류할 수 있을 것이다. 하지만 feature가 100개, 1000개 점점 많아진다면 분할하는 횟수가 엄청나게 많아질 것이다. 트리는 모든 terminal node의 purity가 100%일 때까지 분할 할 것이다. 각 node에 한 종류의 class만 존재할 때 종료를 하게 되고 이를 full tree라고 부르며 이렇게 되면 overfitting의 문제가 생기고 generalization 성능이 저하되는 현상이 발생한다.\n우리가 원하는 것은 학습시킨 모델이 다른 데이터에 대해서도 성능이 높게 나오길 원한다. 즉, generalization 성능이 높아야 된다.\ngeneralization 성능이 저하된다는 것은 다른 데이터에 대해서는 성능이 낮다는 것이고 데이터가 조금이라도 달라지면 모델의 성능이 매우 민감하게 반응할 것이다.\nNo Free Lunch! 위에서 말한 overfitting 문제와 관련해서 아주 재밌는 이론이 있다. 이 이론의 핵심은 모든 가능한 문제에 대해 항상 최고 성능을 보이는 단일한 알고리즘은 존재하지 않는다는 것이다.\n어떤 알고리즘 A가 특정 문제 유형에서 알고리즘 B보다 더 좋은 성능을 낸다면, 반드시 알고리즘 B가 더 좋은 성능을 내는 다른 문제 유형이 존재한다는 것이다.\n하지만 위에서 말했듯이 우리가 원하는 건 다른 데이터에 대해서도 성능이 좋길 원한다. 우리는 모델이 우리가 해결하고자 하는 문제에 대해서도 성능이 좋길 바라고 모델이 보지 못한 데이터에 대해서도 성능이 좋길 바란다.\n이 이론에 따르면 이런 모든 데이터에 대해서 성능이 좋은 알고리즘은 없다는 것이다. 그렇다면 진짜 말하고자 하는 바가 무엇이고 우리에게 어떤 교훈을 주려는 것일까?\n다양한 모델을 시도해야 한다. 특정 문제에 어떤 알고리즘이 가장 적합한지 미리 알 수 없으므로, 여러 모델을 실험하고 성능을 비교 평가하는 과정이 필수적이다. 문제에 대한 이해가 우선이다. 성공적인 ML을 위해서는 데이터와 해결하려는 문제를 깊이 이해하고 그 특성에 맞는 알고리즘을 선택하는 것이 중요하다. 하이퍼파라미터 튜닝의 중요성 같은 알고리즘이라도 하이퍼파라미터를 어떻게 설정하느냐에 따라 성능이 크게 달라질 수 있다. 데이터에 맞게 모델을 세밀하게 조정하는 과정이 필요하다. 어쩃든 결국 공짜 점심 이론이 말하는 건 어떤 문제든 완벽하게 해결하는 단 하나의 만능 모델은 없다라는 것이다.\n이것이 바로 Ensemble 기법의 출발점이다. Ensemble은 \u0026ldquo;공짜 점심은 없다\u0026quot;는 현실을 인정하고, 여러 모델을 협력시켜 최적의 점심을 만들어내자는 것이다.\n2. Ensemble Ensemble(앙상블)는 프랑스어로 함께, 동시에 라는 뜻을 가지고 있다. 또한 영어로는 합창단, 무용단 등을 의미한다.\n이 단어의 뜻에서도 알 수 있 듯이 Machine Learning에서 여러 개의 모델을 협력시키는 것이다.\n하나의 모델은 데이터의 변화에 민감할 수 있지만 다양한 관점에서 학습한 여러 개의 모델의 결과를 종합할 경우 좀 더 강건하고 정확한 결과를 얻을 수 있다.\nBias-Variance Trade-off 여기서 더 나아가면 Ensemble의 목적은 Variance의 감소에 의한 오류를 감소시키는 것이다.\n높은 Bias는 모델이 너무 단순해서 데이터의 패턴을 제대로 학습하지 못하는 상태(Underfitting)이다.\n화살들이 과녁의 정중앙에서 멀리 떨어진 곳에 일관되게 모여있는 경우이다. 조준 자체가 잘못된 것이다.\n높은 Variance는 모델이 훈련 데이터에 너무 과도하게 맞춰져, 새로운 데이터에 대한 예측이 매우 불안정한 상태(Overfitting)이다.\n화살들이 과녁의 정중앙 주변에 있긴 하지만, 매우 넓게 흩어져 있는 경우이다. 쏠 때마다 결과가 크게 흔들리는 것이다.\n모델의 복잡도를 높이면(Decision Tree의 깊이를 깊게 하면) 훈련 데이터를 더 세밀하게 학습하여 Bias는 낮아지지만, 데이터의 노이즈까지 학습하여 Varinace는 높아진다.\n반대로 모델을 너무 단순하게 만들면 Variance는 낮아지지만, 데이터의 패턴을 놓쳐 Bias는 높아진다.\n이 둘은 시소와 같아서, 하나를 낮추면 다른 하나가 올라가는 경향이 있다. 이것이 바로 Bias-Variance Trade-off이다. 머신러닝의 목표는 이 둘의 균형을 잘 맞춰 Total Error를 최소화하는 것이다.\n자세한 수학적인 증명은 다음에\u0026hellip;\n그렇다면 Random Forest는 어떻게 이 문제를 해결하는가?\nRandom Forest의 기본 구성 요소는 Deicision Tree이다.\n깊이가 깊은 Decision Tree는 훈련 데이터를 거의 완벽하게 학습할 수 있다, 즉 Bias가 매우 낮다. 하지만 데이터의 작은 변화에도 구조가 크게 바뀌고 예측이 심하게 흔들리는, Variance가 매우 높은 모델이다.\nRandom Forest는 바로 이 낮은 Bias라는 장점을 유지하면서 높은 Variance라는 치명적인 단점을 해결하려고 한다.\n3. Random Forest의 두 가지 전략 Enmsemble의 Diversity(다양성) 확보를 위해 두 가지 전략이 있다.\nBootstrapp Aggregating (Bagging) Ensemble의 각 멤버(모델)은 서로 다른 학습 데이터셋을 이용한다.\n각 데이터셋은 **sampling with replacement(복원 추출)**을 통해 원래 데이터의 수만큼의 크기를 갖도록 샘플링한다.\n개별 데이터셋을 Bootstrap이라고 부른다.\n이론적으로 하나의 관측치가 하나의 Bootstrap에 한 번도 선택되지 않을 확률은 $$ p = \\left( 1 - \\frac{1}{N} \\right)^{N} \\rightarrow \\lim_{N \\to \\infty} \\left( 1 - \\frac{1}{N} \\right)^{N} = e^{-1} = 0.368 $$\nBagging은 평균을 통해 Bias를 줄인다. 여러 개의 값을 측정해서 평균을 내면 원래 값에 더 가까워진다는 통계적 원리를 이용한다.\n서로 다른 훈련 데이터 샘플을 이용해 수백 개의 독립적인 Decision Tree를 만든다. (각 트리는 여전히 Variance가 높다) 예측 시, 이 모든 트리의 예측 결과를 모아 평균을 내거나 다수결 투표를 한다. A 트리의 예측 오류와 B 트리의 예측 오류는 서로 다른 방향을 튈 가능성이 있따. 따라서 Random Forest는 수백 개의 트리를 만들고 예측을 평균 내면, 이러한 개별적인 오류들이 서로 상쇄되면서 숲 전체의 Variance가 줄어들게 된다.\nRandom Subspace t 번쨰 트리를 살펴보자\n원래 변수들 중에서 모델 구축에 쓰일 입력 변수를 무작위로 선택한다. 선택된 입력 변수 중에 분할될 변수를 선택한다. 이러한 과정을 full-grown tree가 될 때까지 반복한다. Decision Tree의 분기점을 탐색할 때, 원래 변수의 수보다 적은 수의 변수를 임의로 선택하여 해당 변수들만 고려 대상으로 한다.\n만약 모든 트리가 같은 데이터와 같은 특성을 보고 학습한다면, 결국 비슷한 모양의 트리들만 만들어질 것이다. 이런 비슷한 트리들의 예측을 평균 내봤다 Variance 감소 효과는 미미할 것이다.\n다시 정리해보자.\n각 트리의 노드를 분할할 때마다, 전체 feature 중 일부만 무작위로 선택한다. 선택된 feature들 중에서만 최적의 split criterion을 찾도록 강제한다. 이 과정을 통해 각 트리들은 서로 다른 feature를 기반으로 성장하게 되어 모양이 더욱 다양해진다.(상관관계 감소) 이렇게 서로 다른 개성을 가진 트리들의 예측을 평균 낼 때 Variance 감소 효과는 커진다.\n4. Generalization Error 각각의 개별 트리는 Overfitting될 수 있다.\nRandom Forest는 트리수가 충분히 많을 때 Strong Law of Large Numbers에 의해 Overfitting되지 않고 그 error는 limiting value에 수렴된다.\n$$ \\text{Generalization error} \\leq \\frac{\\bar{\\rho}(1 - s^2)}{s^2} $$\n$\\bar{\\rho}$: Decision Tree 사이의 평균 상관관계 $s$: 올바로 예측한 트리와 잘못 예측한 트리 수 차이의 평균 개별 트리의 정확도가 높을수록 $s$는 증가한다.\nBagging과 Random Subspace는 각 모델들의 독립성, 일반화, 무작위성을 최대화 시켜 모델간의 상관관계 $\\rho$를 감소시킨다.\n개별 트리의 정확도, 독립성이 높을수록 Random Forest의 성능이 높아진다.\n5. Out of bag (OOB) Decision Tree는 직관적이고 사람이 해석 가능하다. Random Forest는 Decision Tree를 수백 개를 만들어낸다. 그리고 수백 개의 트리들을 평균을 낸다.\n이렇게 수백 개의 트리를 만들어내면 어떻게 이 알고리즘의 성능이 좋은지 안 좋은지를 평가할 수 있을까?\n또한, 직관적이고 해석 가능한 Deicision Tree의 장점이 사라지게 되는게 아닐까?\nOOB의 가장 중요한 용도는 모델 성능을 검증하는 것이다.\nOOB 검증의 전체 과정 1단계. 모델 훈련 Random Forest는 수백 개의 Decision Tree로 이루어진 숲이다. 각 트리를 훈련시킬 때, 전체 훈련 데이터가 아닌 Bootstrap Sample을 사용한다.\nsampling: 전체 훈련 데이터(100개)에서 중복을 허용하여 무작위로 데이터 100개를 뽑는다. OOB 샘플 발생: 이 과정에서 어떤 데이터는 여러 번 뽑히고, 어떤 데이터는 한 번도 뽑히지 않는다. 여기서 뽑히지 않는 데이터가 나올 확률은 위에서 설명했듯이 약 0.37이다. 예시 트리 1의 훈련 데이터(Bootstrap sample)에는 데이터 #5가 포함되지 않았다. 데이터 #5는 트리 1의 OOB 샘플이다. 트리 2의 훈련 데이터에는 데이터 #5가 포함되었다. 데이터 #5는 트리 2의 훈련 샘플(In-Bag)이다. 이 과정을 숲의 모든 트리에 대해 반복한다. 결과적으로, 모든 데이터는 어떤 트리에게는 훈련 데이터로 쓰이고, 다른 트리에게는 OOB 샘플로 남게 된다.\n2단계. OOB Prediction 이제 훈련이 끝났으니, 모델의 성능을 검증할 차례이다.\n여기서 단 한 가지 중요한 규칙이 있다.\n규칙: 각 데이터는 자신을 학습하는 데 사용하지 않았던 트리들에게만 가서 예측을 받는다. 예시 숲에 있는 500개의 트리가 있고 이 트리들 중에서 훈련할 때 데이터 #5를 사용하지 않았던 트리들(OOB 샘플로 가졌던 트리들)을 모두 찾아낸다.(트리1, 3, 6.. 499번 등 약 200개의 트리들을 찾았다고 해보자.) 이 트리들에게만 데이터 #5를 보여주고 예측을 진행한다. 이 트리들의 예측 결과를 종합한다(classification이면 다수결, regression이면 평균) 이렇게 얻은 최종 결론이 바로 데이터 #5에 대한 OOB 예측값이다. 이 과정을 훈련 데이터에서 모든 개별 데이터에 대해 반복한다.(데이터 #1부터 #100까지 각각의 OOB 예측값을 모두 구한다.)\n3단계. OOB Score 계산 이제 모든 데이터에 대한 OOB 예측값과 실제 정답을 확보했다. 마지막으로 이 둘을 비교하여 최종 성능을 계산한다.\n데이터 #1의 OOB 예측값 \u0026lt;-\u0026gt; 데이터 #1의 실제 정답 데이터 #2의 OOB 예측값 \u0026lt;-\u0026gt; 데이터 #2의 실제 정답 \u0026hellip; 데이터 #100의 OOB 예측값 \u0026lt;-\u0026gt; 데이터 #100의 실제 정답 이 비교 결과를 바탕으로 모델의 전체적인 성능 지표(정확도, 정밀도, MSE 등)을 계산한다. 이렇게 계산된 최종 성능 지표가 바로 OOB Score이다.\nOOB Score는 모델이 처음 보는 데이터에 대해서 얼마나 잘 작동할지를 나타내는 신뢰할 수 있는 generalization 성능을 추정할 수 있다.\n6. Featrue Importance OOB를 통해 변수의 중요도를 구할 수 있다.\nRandom Forest에서 변수의 중요도가 높다면\nRandom permutation 전-후의 OOB Error 차이가 크게 나타나야 한다. Random permutation 전은 OOB data에서 어떤 변수의 관측치들을 섞기 전이고 이에 대한 OOB error $p_i$를 계산한다. Random permutation 후는 OOB data에서 어떤 변수의 관측치들을 무작위로 섞고 OOB error $e_i$를 계산한다. 차이를 구한다 $d_i^m = p_i^m - e_i^m$, (기준 성능) - (뒤섞은 후 성능) 이 차이가 클수록 그 변수가 모델의 예측에 중요하게 사용되었다는 의미다. 그 차이의 편차가 적어야 한다. m번째 트리에서 변수 i에 대한 Random permutation 전후 OOB error의 차이는 $$ d_i^m = p_i^m - e_i^m $$\n전체 트리들에 대한 OOB error 차이의 평균 및 분산 $$ \\bar{d}i = \\frac{1}{m} \\sum{i=1}^{m} d_i^m, \\quad s_i^2 = \\frac{1}{m-1} \\sum_{i=1}^{m} (d_i^m - \\bar{d}_i)^2 $$\ni번째 변수의 중요도 $$ v_i = \\frac{\\bar{d}_i}{s_i} $$\n이렇게 변수의 중요도를 측정하고 barplot으로 나타낸다면\nConclusion Random Forest는 Ensemble 기법인 Bagging 사용하여 단일 Decision Tree의 높은 Variance를 줄인다.\nVariance가 줄으면서 모델은 새로운 데이터에 대해서 민감하게 반응하지 않고 강건하고 generalization 성능이 높은 모델이 된다.\n또한, 단일 Decision Tree는 2차원으로 생각했을 때 이진 경계면을 생성하지만 Random Forest에서 수백 개의 Decision Tree를 만들고 모든 트리에 대해서 평균을 내므로 예측값이 연속형의 스코어 가깝게 생성된다.\n필자도 AI 대회에서 다양한 알고리즘에 대한 테스트를 수행하기 전에 Random forest를 baseline으로 먼저 적용해본다.\n학습 hyperparameter에 대해서 variance가 크게 낮은 모델이기 때문에 왠만한 문제에 있어서 좋은 성능을 보이기 때문이다.\n","permalink":"https://xooyong.github.io/posts/2025-07/2025-07-12-random_forest/","summary":"\u003ch2 id=\"1-단일-deicision-tree의-한계\"\u003e1. 단일 Deicision Tree의 한계\u003c/h2\u003e\n\u003cp\u003e저번 글에서 Decision Tree에 대해서 자세히 알아보았다. Decision Tree는 직관적이고 사람이 해석 가능하며 매우 좋은 알고리즘인 건 분명하다.\n하지만 몇 개의 문제가 있다.\u003c/p\u003e\n\u003ch3 id=\"단일-decision-tree의-가장-큰-문제---overfitting\"\u003e단일 Decision Tree의 가장 큰 문제 - Overfitting\u003c/h3\u003e\n\u003cp\u003e데이터가 매우 복잡하고 해석하기 어려울 때 Deicision Tree에서 분할은 어떻게 될까? 데이터의 feature가 1개, 2개, 10개 이하 이렇게 feature의 수가 적다면 데이터를 분할하기 쉽고 빠르게 데이터를 분류할 수 있을 것이다. 하지만 feature가 100개, 1000개 점점 많아진다면 분할하는 횟수가 엄청나게 많아질 것이다. 트리는 모든 terminal node의 purity가 100%일 때까지 분할 할 것이다. 각 node에 한 종류의 class만 존재할 때 종료를 하게 되고 이를 \u003cstrong\u003efull tree\u003c/strong\u003e라고 부르며 이렇게 되면 \u003cstrong\u003eoverfitting\u003c/strong\u003e의 문제가 생기고 generalization 성능이 저하되는 현상이 발생한다.\u003c/p\u003e","title":"Random Forest"},{"content":"1. Deicison Tree Decision Tree는 질문이 있고 이 질문에 대해서 Yes, No로 대답하는 문제와 같다.\n이렇게 특정 기준(질문)에 따라 데이터를 나누는 것이 Decision Tree이다. 또한 이렇게 데이터를 분류하는 방식의 가장 큰 장점은 직관적 이라는 것이다. 직관적이라는 것은 사람이 해석 가능하다는 것이다.\n우리가 GPT, Claude와 같은 생성형AI, 복잡한 Deep Learning Model들은 사람이 해석하기가 힘들다는 것이다. 물론 완전히 해석하지 못하는 것은 아니지만, 가끔씩 왜 이런 결과가 나왔을지 추측하기 힘든 경우가 많다.\nML은 사람이 최대한 개입하지 않고 스스로 기계가 학습하게 하는 것이다. 그렇다면 이 알고리즘을 가지고 어떻게 자동으로 학습하게 할 수 있을까? 또한 supervised learning에서는 classification과 regression 문제가 있다 먼저 Classification Tree를 알아보자.\n2. Terminologies Root node: 처음 시작 노드 - child node만 가지고 있음 Parent node: 분할 전의 노드 Child node: 분할 후의 노드 Leaf nodes: 마지막 노드 - 결론 split criterion: 분할 할 때 사용한 변수와 값 - 질문, 분할 기준 데이터는 Root node에서부터 시작해서 Leaf node 까지 split criterion을 거치면서 분류될 것이다.\nExample Decision Tree에서 데이터가 분류되는 과정을 간단하게 살펴보자. 먼저 데이터는 \u0026ldquo;오늘 운동할까?\u0026ldquo;를 결정하는 것이다.\nindex X1: 날씨 X2: 온도 X3: 시간 운동여부(label) 0 맑음 높음 오전 ✅ 운동 1 맑음 높음 오후 ❌ 안함 2 맑음 낮음 오전 ✅ 운동 3 맑음 낮음 오후 ✅ 운동 4 비 높음 오전 ❌ 안함 5 비 높음 오후 ❌ 안함 6 비 낮음 오전 ❌ 안함 7 비 낮음 오후 ❌ 안함 이렇게 정형 데이터가 있을 때 날씨(X1), 온도(X2), 시간(X3)은 feature가 될 것이고, 운동여부는 label이다. 데이터 날씨(X1)을 기준으로 분류해보자. split criterion은 \u0026ldquo;날씨가 맑음?\u0026ldquo;으로 할 수 있을 것이다. 그렇다면 다음 node로 index 0, 1, 2, 3 날씨 맑음 데이터와 4, 5, 6, 7 비 데이터가 분류될 것이다. 그 다음은 온도(X2)를 기준으로 분류해보자 split criterion은 \u0026ldquo;온도가 높음?\u0026ldquo;으로 할 수 있다. 그렇다면 날씨가 맑은 데이터 0, 1, 2, 3에서 온도가 높은 데이터는 0, 1 데이터이므로 또 다음 node로 분류될 것이다. 이렇게 계속해서 leaf node에 도달할 것이다.\n날씨가 맑음?\r/ \\\rYES NO\r/ \\\r온도가 높음? 운동 안함 ❌\r/ \\\rYES NO\r/ \\\r시간이 오전? 운동함 ✅\r/ \\\rYES NO\r/ \\\r운동함 ✅ 운동 안함 ❌ 자 여기서 우리는 규칙을 추출할 수 있다.\n비가 오면 -\u0026gt; 운동 안함 맑고 온도가 낮으면 -\u0026gt; 운동함 맑고 온도가 높고 오전이면 -\u0026gt; 운동함 맑고 온도가 높고 오후면 -\u0026gt; 운동 안함 4번에서 맑고 온도가 높고 오후면 왜 운동을 안할까? 생각해보면 당연하게 온도가 높고 오후면 사람은 운동하기가 싫어질 것이다.\n이런 식으로 데이터에서 자동으로 규칙을 찾아내는 것이 Decision Tree의 핵심이다.\n3. Decision Tree - Geometric Perspective 우리는 지금까지 Decision Tree를 우리한테 익숙한 트리 형식으로 보았다. 하지만 우리는 기하학적으로 이해할 수 있다.\n이런식으로 데이터들을 split criterion(ex: 온도 \u0026gt; 25)로 데이터들을 분류하고 이를 2차원애서 하나의 직선으로 생각할 수 있다.\n또한, 최종 결과를 보면 영역이 빨간색과 초록색으로 나누어 지는 걸 볼 수 있다. 영역이 이렇게 나누어지는 기준은 label이고 다수결로 결정된다.\n데이터들이 Decision Tree로 들어가고 split criterion을 기준으로 분류된다. 그리고 분류가 다 끝나면 데이터들은 leaf node에 있을 것이다. leaf node를 확인해보았을 때 label이 운동 안함(빨간색)이 많다면 그 leaf node의 label은 운동 안함이 될 것이다. 그러니까 온도 \u0026lt; 25인 데이터들은 모두 운동 안함으로 학습한 것이고, 나중에 어떤 데이터가 들어왔을 때 온도가 25도 이하이면 운동 안함으로 분류할 수 있을것이다.\nclassification에서는 이렇게 label이 운동을 하냐, 안하냐, 개냐, 고양이냐 카테고리로 분류가 된다. 하지만 실제로 Decision Tree에서 leaf node에서의 class를 결정할 때 확률로 결정된다.\n어떤 leaf node에 데이터가 3개가 있고 각 데이터의 label이 x1(운동함), x2(운동 안함), x3(운동 안함) 이렇게 있다고 가정하자.\n그렇다면 여기서 운동함의 확률은 1/3, 운동 안함의 확률은 2/3로 표현할 수 있다.\n따라서 어떤 데이터 t1을 이렇게 학습된 Decision Tree에 넣고 이 leaf node에 도착했다면, 그 데이터 t1은 운동 안함이 될 것이고, 값은 2/3가 된다. 이 값은 나중에 loss를 계산할 때 사용될 것이다.\n또한, 확률로 계산하면 0.67, 0.33과 같이 더 세밀하게 표현할 수 있고, 단순히 우리가 분류할 때 확률이 0.5 이상이면 좋고, 0.5 이하이면 나쁘다라고 분류하면 문제가 발생할 수 있다. 확률이 0.51이여도 좋음이라고 분류되기 때문에 이는 다시 한번의 검증이 필요할 수도 있다.\n4. split criterion 데이터가 split criterion을 기준으로 나누어지고 leaf node에 도달했을 때 label에 따라 다수결로 결정되고 이는 결국 확률로 계산된다는 것을 알았다. 그러면 split criterion 이 분할 기준을 어떻게 정하고 찾는 것일까?\n원래 아주 기본적으로는 위에서 본 정형 데이터에서 어떤 featrue를 기준으로 그 값들 사이에 선을 하나씩 그어보는 것이다.\nX1 0.1 0.4 0.6 아주 간단하게 이런 데이터가 있을 때 0.1, 0.4 사이를 기준으로 나누어 보고, 0.4, 0.6 사이를 기준으로 나누어 보고\u0026hellip; 이런 식으로 분류될 것이다.\n하지만 데이터가 매우 커진다면 이런 식으로 하나씩 선을 그어 보고 최적의 split criterion을 찾는다면 매우 많은 시간과 computing power가 필요할 것이다.\nOptimization 우리는 최적의 split criterion을 찾으려면 먼저 최적화(Optimization)을 알아보자.\n최적화(Optimization) 문제란 어떤 목적함수(Objective function)의 함수값을 최적화(최대화 또는 최소화)시키는 파라미터(변수) 조합을 찾는 문제이다.\n최적화(optimization) 문제는 크게 최대화(maximization) 문제와 최소화(minimization) 문제로 나눌 수 있고, 목적함수가 이윤, 점수 등인 경우에는 최대화 문제, 비용, 손실, 에러 등인 경우에는 최소화 문제가 된다.\n자 여기서 우리가 최적화할 기준 함수는 Loss Function으로 정의될 것이고, 이 기준 함수를 최소화 또는 최대화 하는 방향으로 ML 알고리즘을 학습시킨다.\n결론적으로, 기준 함수(Loss Function)을 최대화 또는 최소화 하는 Split criterion을 찾는 것이다.\n5. 그래서 최적의 split criterion을 찾는 방법은? Entropy Entropy는 어떤 공간의 불확실성을 측정한다.\n다르게 표현하면 Entropy는 node의 **불순도(impurity)**를 측정하는 것이다.\n$$ H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i) $$\nH(S): 집합 S의 엔트로피 c: 클래스 개수 p_i: 클래스 i의 비율 Entropy가 높다라는 의미는 해당 node에서 class의 비율이 불확실하다 라는 것이고, class가 0과 1이 있을 때 50:50으로 있다는 것이다. 뭐가 나올 지 모르는 이 상태를 우리는 불확실성이 높다라고도 한다.\nEntropy가 낮다라는 의미는 해당 node에서 class의 비율이 순수하다는 것이고 100:0 또는 0:100 이라면 Entropy=0 이라는 것이다.\nInformation Gain 위 사진에서 선이 split criterion이 되는 것이고 우리는 저 선을 보았을 때 위 영역은 분류가 잘됬고 밑 영역은 class들이 섞여있긴 하지만 그래도 분류가 괜찮게 됬다고 생각할 수 있다. 하지만 컴퓨터는 이렇게 분류를 했을 때 어떻게 분류하는게 좋은 것인지 모른다.\n따라서 우리는 모델한테 어떻게 선을 그어야 분류가 좋은 것인지 기준, 규칙 등을 주어야 한다.\n우리는 Information Gain(정보 이론)을 통해서 정말 데이터들이 잘 나누어 졌을까를 측정할 수 있다.\n$$ \\max_{j,s} \\text{IG}(j,s) = H(S) - \\left[ \\frac{|L_1|}{|S|} H(L_1) + \\frac{|L_2|}{|S|} H(L_2) \\right] $$\n$j$: Feature (온도, 습도) $s$: Split threshold (분할점) $S$: Root Node (전체 데이터) $L_1$: Left childe node (왼쪽 분할 영역) $j$: Right child node (오른쪽 분할 영역) $H(S)$: Root Node의 Entropy $H(L_1)$, $H(L_2)$: 각 분할 영역의 Entropy 목적 함수는 IG가 되는 것이고 이걸 최대화 하는 것이 목표다.\n위에서 볼 수 있듯이 온도를 기준으로 세로로 선을 그어서 분할했을 떄는 각 분할된 node에서의 Entropy가 크다. Entropy가 크다 -\u0026gt; 해당 node에서의 불확실성이 크다 -\u0026gt; class의 분포가 50:50에 가깝다.\n하지만 습도를 기준으로 가로로 선을 그어서 분할하면 각 node에서의 Entropy가 0이 된다. Entropy가 작다 -\u0026gt; 해당 node에서의 불확실성이 낮다. -\u0026gt; class가 섞여있지 않고 하나의 class만 있다.\n다시 정리해보자. Entropy는 어떤 공간의 불확실성을 측정한다. class의 비율이 반반 섞여있다면 불확실성이 크다(Entropy가 크다)라고 표현하고 class가 하나만 있다라고 하면 불확실성이 낮다(Entropy가 작다)라고 표현한다. 우리는 여기서 공간을 분할했을 때(트리를 분기했을 떄) 분할 전과 분할 후의 불확실성을 측정해서 IG를 최대화 하는 방향으로 최적화를 할 것이다.\nIG가 크가라는 것을 생각해보자. 우리는 IG값을 크게 하는 것이 목표이고 IG값일 클수록 분할이 잘 되었다고 해석할 수 있다.\nRoot node에서의 Entropy가 1이라고 해보자. 그리고 분할 된 공간 L1 node, L2 node가 있을 때 두 공간의 Entropy가 0이라고 해보자. 가중치를 제외하고 단순하게 계산한다면 $H(S)=1-(0+0)=1$ 이므로 이것은 분할이 아주 잘 되었다고 할 수 있다. 왜냐하면 Entropy가 크다라는 것은 해당 공간(node)의 불확실성이 크다는 것이고 공간을 분할했을 때 각각의 공간(L1, L2)의 Entropy가 작아졌고, 이 차이를 구하면 IG값은 당연히 큰게 분할이 잘 됬다고 해석할 수 있다.\nGini Index Entropy 말고도 더 좋은 분할 기준이 있다.\n$$ \\text{Gini}(S) = 1 - \\sum_{i=1}^{c} p_i^2 $$\nCPU는 log 연산을 직접적으로 하지 못한다. 대신 테일러 급수나 룩업 테이블을 사용해서 계산을 한다.\nx를 2^k × m 형태로 분해 (k는 정수, 1 ≤ m \u0026lt; 2) 룩업 테이블에서 log(m) 근사값 찾기 log₂(x) = k + log₂(m) 계산 여러 번의 덧셈/곱셈 반복 이런 식의 연산을 하기 때문에 Entropy의 log 함수에서 연산이 느려진다. 하지만 gini는 제곱 연산을 하기 때문에 log 연산보다 훨씬 빠르다. 제곱 연산은 단순 곱셈 2번이기 때문에 시간 복잡도 O(1)만에 처리 할 수 있다. 또한 값의 표현 범위도 Entropy는 0과 1사이 이지만 gini는 0과 0.5사이에서 표현된다.\n6. Regression Tree Classification에서는 데이터의 label 형태가 categorical 했다. 이는 데이터가 0이냐 1이냐, 개냐 고양이냐, 이렇게 딱 떨어지게 분류할 수 있다는 뜻이다.\nRegression 문제는 데이터가 이렇게 카테고리컬 하지 않다. 어떤 실수로 표현되있다. 이는 금액, 점수 등 이렇게 숫자로 표현된다는 것이다.\nclassification tree에서는 다수결 즉, leaf node에서 가장 많은 label에 따라 예측값 class가 결정됬다.\nregression tree에서는 class들의 형태가 숫자로 되어 있다고 했다. 다수결과 가장 비슷한 방법인 평균을 사용하는 것이다.\nleaf node 내에 데이터가 3개가 있고 각 데이터의 label이 0.1, 0.2, 0.3이라고 생각해보자.\n이것의 평균은 (0.1 + 0.2 + 0.3) / 3 = 0.167이 될것이다.\n이 뜻은 어떤 다른 데이터가 이 regression tree로 들어오고 위 leaf node에 들어왔을 때 예측값이 0.167이 된다는 것이다.\nMSE or Variance regression tree에서 split criterion을 구할 때 IG 사용한다. 하지만 분할 기준은 Entropy가 아닌 MSE를 사용한다.\n또한 이 MSE로 생각했을 때 우리는 분산(Variance)로도 접근할 수 있다.\n$$ \\text{MSE}(S) = \\frac{1}{|S|} \\sum_{i=1}^{|S|} (y_i - \\bar{y})^2 $$\n이를 IG에 적용시키면\n$$ \\text{MSE Reduction} = \\text{MSE}(S) - \\left[ \\frac{|L_1|}{|S|} \\text{MSE}(L_1) + \\frac{|L_2|}{|S|} \\text{MSE}(L_2) \\right] $$\n분할 전 MSE는\n분할 후\nclassification과 같이 목적 함수를 최대화 하는 방향으로 최적화 할 것이다.\nConclusion Decision Tree의 가장 큰 장점은 직관적이라는 것이고 사람이 해석 가능하다는 것이다.\n사람이 해석 가능하다면 우리는 왜 이런 결과가 나오고 이런 결과가 나오기까지의 과정을 사람이 이해할 수 있다는 것이다. 이는 문제와 원인을 분석해서 모델을 더 좋은 방향으로 분석하고 개선할 수 있다는 것이다.\n하지만 이런 Decision Tree에서도 문제가 있다. 단일 Decision Tree로는 깊이가 깊어지게 되면 그 만큼 트리를 많이 분할 한다는 것이고 이렇게 되면 트리는 noise까지 학습하게 되어 Overfitting문제에 직면하게 된다.\n이를 해결하기 위해 Purning을 통해서 너무 분할이 되지 않게 막을 수 있다. 또한 이왕이면 복잡한 모델보다 단순한 모델을 구축하는 것이 좋을 것이다.\n또 다른 문제로는 불안정성(High Variance)가 있다. 이는 훈련 데이터(train data)가 조금만 바뀌어도 완전히 다른 트리를 생성할 수 있다.\n그리고 마지막으로 우리가 트리를 2차원으로 표현할 수 있어 사람이 해석하기 쉽지만, 대각선이나 곡선 경계를 표현하지 못한다는 것이다.\n이러한 문제들을 해결하기 위해 나온 Random Forest는 다음에 알아보도록 하자.\nRefernece https://github.com/pilsung-kang/Business-Analytics-ITS504-/tree/master ","permalink":"https://xooyong.github.io/posts/2025-07/2025-07-07-decision_tree/","summary":"\u003ch2 id=\"1-deicison-tree\"\u003e1. Deicison Tree\u003c/h2\u003e\n\u003cp\u003eDecision Tree는 질문이 있고 이 질문에 대해서 Yes, No로 대답하는 문제와 같다.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\r\n  \u003cimg src=\"/images/Decision_Tree/decision-tree.png\" width=\"50%\" alt=\"Decision Tree\"\u003e\r\n\u003c/p\u003e\r\n\u003cp\u003e이렇게 특정 기준(질문)에 따라 데이터를 나누는 것이 Decision Tree이다. 또한 이렇게 데이터를 분류하는 방식의 가장 큰 장점은 \u003cstrong\u003e직관적\u003c/strong\u003e 이라는 것이다. 직관적이라는 것은 사람이 해석 가능하다는 것이다.\u003c/p\u003e\n\u003cp\u003e우리가 GPT, Claude와 같은 생성형AI, 복잡한 Deep Learning Model들은 사람이 해석하기가 힘들다는 것이다. 물론 완전히 해석하지 못하는 것은 아니지만, 가끔씩 왜 이런 결과가 나왔을지 추측하기 힘든 경우가 많다.\u003c/p\u003e","title":"Decision Tree"},{"content":"1. Machine Learning Machine Learning이란 뭘까? 1959년, 아서 사무엘은 ML을 이렇게 정의했다.\n\u0026ldquo;기계가 일일이 코드로 명시하지 않은 동작을 데이터로부터 학습하여 실핼할 수 있도록 하는 알고리즘을 개발하는 연구 분야이다.\u0026rdquo;\n쉽게 말하면 기계가 스스로 학습하는 것이라고 할 수 있겠다. 그렇다면 기계는 어떤 것을 학습하고 어떻게 학습할 수 있을까? 먼저 학습을 하기 위해서 어떤 요소들이 필요한지 살펴보자. 나는 기계가 학습하기 위해서, 크게 3가지 요소로 나누어 진다고 생각한다.\nData ML 개념 - Supervised Learning, Unsupervised Learning, Reinforcement Learning\u0026hellip; Algorithm - Linear Regression, Tree, Graph, Neural Networks\u0026hellip; 모델은 데이터를 기반으로 학습한다. 데이터가 없다면 모델은 아무것도 할 수가 없다. 데이터의 품질, 양에 따라서 모델의 성능이 크게 좌우된다. 많은 양의 데이터도 중요하지만 데이터의 품질이 중요하다고 생각된다. 데이터가 품질이 좋고 해당되는 분야에 대해서 잘 표현되어 있다면 모델은 이를 통해 좋은 정보와 특징들을 학습하고 우리에게 원하는 답을 찾아 줄 것이다.\nML 개념은 ML에서 가장 중요한 요소이다. 모델의 학습 방식이라고도 할 수 있지만 데이터의 형태라고도 할 수 있다. Supervised learning에서는 Classification, Regression으로 나누어 지고, Unsupervised learning에서는 Clustering, Anomaly Detection 등으로 나누어 진다. 이렇게 데이터의 형태에 따라서 ML을 하는데 있어서 아키텍쳐, 방법, 구성 등 모든 것이 바뀔 수 있다.\n데이터가 주어졌고 데이터의 형태를 우리가 알았다. 이제 마지막으로 Algorithm이 필요하다. Algorithm은 모델이라고도 할 수 있다. 모델은 데이터를 입력받고 예측값을 출력한다. 이 과정 속에서 모델은 지속적으로 학습하고 틀린 문제를 보완할 것이다. 즉 parameter를 업데이트하는 절차를 따른다.\n자 이렇게 ML을 하기 위한 모든 요소들이 갖추어졌다. 그렇다면 Machine Learning에서 Machine은 무엇을 배운다는 것일까? 가장 대표적인 예를 살펴보자\n$$ y=f(x) $$\nML은 입력 데이터 x로부터 출력 y를 잘 예측하는 함수 f를 찾는 과정이라고 할 수 있다. 여기서 머신러닝의 목적은 궁극적으로 f를 찾는 것이다. 데이터셋 $(x_1, y_1), (x_2, y_2), \u0026hellip;, (x_n, y_n)$으로부터 f의 형태와 파라미터를 찾아서, 새로운 x에 대해 y를 잘 예측하는 모델을 만드는 것이다. Linear Regresssion에서는 $f(x)=wx+b$, Neural Network에서는 $f(x)$가 여러 층의 비선형 함수 조합, Decision Tree에서는 $f(x)$가 조건문 분기 구조가 될 것이다.\n그렇지만 이 $y=f(x)$라는 해석은 기본적으로 Supervised Learning에 해당하는 해석이다. 그리고 Supervised Learning은 ML에서의 핵심 학습 방식이다.\n2. Supervised Learning Supervised Learning의 대표적인 정의를 보자.\nSupervised learning is the process of trying to infer from labeled data the underlying function that produced the labels associated with the data\nSupervised learning은 레이블이 있는 데이터로부터 해당 레이블을 생성한 근본적인 함수를 추론하려는 과정이다.\nlabeled data라는 것은 뭘까? 정답이 있는 데이터를 말하는 것이고 label은 정답이다. 그리고 근본적인 함수는 f를 말하는 것이다. 위에서 말했던 내용과 같다. 결국 labeled data $D={(x_1, y_1), (x_2, y_2), \u0026hellip; ,(x_n, y_n)}$에 대해서 입력 $x$를 받아 출력 $y$(label)을 예측하는 근본적인 함수 f를 학습하고 가장 $y$(label)을 잘 표현하는 f를 찾는 것이다.\n처음에서도 말했듯이 Supervised learning은 크게 Classification, Regression으로 나뉜다. 공통점은 입력 데이터 $x$와 정답 $y$가 주어진 상황에서, 입력으로부터 정답을 예측하는 함수 $f(x)$를 학습하는 것을 목표로 한다.\nClassification은 입력 데이터를 미리 정의된 **클래스(범주)**중 하나로 분류하는 문제이다. 정답 $y$는 이산형(discrete)값이고 예를 들어, 개, 고양이, 양성/음성, 숫자0~9로 표현된다. 문제의 정답은 이렇게 표현되지만 모델이 이를 학습하고 나오는 output은 확률이다. 그러니까 output이 0.5이상이면 개라고 분류하고 0.5 미만이면 고양이라고 분류할 것이다. 확률에 따라서 관측치의 범주를 예측하고 분류한다.\nRegression은 입력 데이터를 기반으로 연속적인 수치값을 예측하는 문제이다. 정답 $y$는 연속형(continuous)값이고 예를 들어, 가격, 온도, 수치 등을 말한다. output은 보통 실수(float)값이다.\nML을 하기 위해서는 어떤 것들이 필요하고 어떤 것을 학습하는지도 알아보았다. 모델은 한 번에 y를 잘 표현하는 f를 바로 찾아내지는 못할것이다. 반복적으로 학습을 하고 예측을 하면서 보완해나간다. 모델의 예측이 정답과 틀렸을 때 어떻게 보완하고 틀렸다는 것을 알 수 있을까? 또는 어떻게 모델의 성능이 좋고 나쁘다는 것을 평가할 수 있을까?\n3. 모델의 성능 평가 기준 먼저 Optimization의 개념을 알아야 한다. Optimization은 어떤 **목적함수(objective function)**의 함수값을 Optimization(maximize 또는 minimize)시키는 parameter(변수)조합을 찾는 문제를 말한다. 여기서 objective function의 구조 및 형태와 이에 따른 찾아야 될 parameter 형태에 따라 다양한 방법이 존재한다. 또한 다루고자 하는 Optimization 문제는 별도의 제약조건이 있는 경우와 없는 경우가 존재한다.\n$$\\underset{x}{minimize} \\quad f(x)$$ subject to $$g_i(x)\u0026lt;=0, i=1,\u0026hellip;,m$$ $$h_j(x)=0, j=1,\u0026hellip;,p$$\n(where $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ is the objective function to be minimized over the $n$-variable vector $x$)\nOptimization 문제는 크게 maximization 문제와 minimization 문제로 나눌 수 있는데 objective function이 이윤, 점수(score) 등인 경우에는 maximization 문제가 되고, 비용(cost), 손실(loss), 에러(error) 등인 경우에는 minimization 문제가 된다.\n일반적으로 ML 모델들은 앞서 설명한 기준 함수를 Objective, Cost, Loss Function으로 정의하고 이 기준 함수를 minimize 또는 maximize 하는 방향으로 모델을 학습시킨다.\n4. 우리의 문제 이렇게 ML에 대해서 간단하게 알아보았다. 우리가 어떤 문제를 해결하고 싶어서 데이터를 수집하고 데이터의 형태를 알아보고 분석하고 이에 맞는 모델을 선택하고 어떻게 평가할 지, 어떻게 최적화할 지 선택하고 정하게 될 것이다. 이 과정 속에서는 우리가 항상 생각하고 고려해야되는 문제들이 있다.\n4-1. 데이터의 문제 데이터는 거짓말을 하지 않는다\u0026hellip; 하지만 현실 세계에서의 데이터는 완벽하거나 정확하지 않고 불확실하고 잘못되거나 지저분한 경우가 많다. 또는 label이 없는 경우가 많다. 모델은 데이터가 없으면 아무것도 할 수 없고 데이터의 품질이 좋으면 자연스럽게 모델의 성능도 높아질 것이다. 모델의 성공 여부는 데이터 품질에 크게 좌우된다.\n먼저, 우리는 품질 좋은 데이터를 얻기 위해, 모델의 성능을 높이기 위해 데이터를 가공하고 정제해야 한다. 데이터 내부에 noise, missing value, 잘못된 labeling 등 이러한 문제들을 해결해야 한다. 데이터의 noise는 관련성이 없거나 무의미한 데이터, 무작위 오류 또는 기본 구조와 추출하고자 하는 진실을 왜곡하는 편차(bias)를 의미한다. missing value(결측치)는 데이터에서 값이 비어있거나 누락된 부분을 말한다. 잘못된 label은 사람의 실수로 인해 발생할 수 있다. 예를 들어 개 이미지가 고양이로 잘못 라벨링되는 등 다양한 방식으로 발생할 수 있다.\n데이터에 편향이 존재할 수도 있다. 특정 인종, 성별, 지역 등으로 인해 불균형한 분포를 이루고 이에 따라 class imbalance가 발생하고 학습된 모델이 차별적으로 예측하거나 어떤 class에 대해서는 예측을 잘 못할 수도 있다. 예를 들면, 개와 고양이 사진이 있는데 개에 대한 사진만 너무 많아서 고양이 대해서는 잘 예측하지 못하는 것이다. 또는 고양이도 개라고 예측할 수도 있다.\n데이터 자체가 부족할 수도 있다. 기업 관점에서 자기들이 서비스하는 부분에서 발생하는 데이터는 귀중한 자산이 되고 차별점이 될 것이다. 이에 따라 우리는 데이터 확보가 어려울 수 있고 또는 의료, 법률 등 민감 분야에서 labeled data 확보가 어렵다. 이렇게 되면 작은 데이터셋은 Overfitting, Underfitting 문제와 직결된다.\n데이터에 label이 없다면 우리가 수동으로 labeling 해줄 수 있다. 하지만 labeling 자체가 시간과 비용이 많이 들고 해당 데이터의 도메인 전문가가 필요할 것이다. 수동 라벨링에는 편향성 위험이 발생한다. 이는 labeler가 무의식적으로 한 class를 다른 class보다 선호할 수 있기 때문이다.\n데이터가 복잡한 형태를 띄고 있을수도 있다. The curse of dimensionality는 데이터의 feature 개수가 관측치의 개수보다 많아져 즉, 차원이 증가할 수록 개별 차원 내 학습할 데이터 수가 적어지는(sparse)현상이 발생한다. 낮은 차원에서 촘촘하게 분포하고 있던 데이터가 차원이 높아질수록 각각의 관측치 사이의 거리가 점점 멀어지게 되고 그 사이에 빈 공간이 생긴다. 컴퓨터 상으로 0으로 채워졌다는 뜻이다. 정보가 없어지고 정보가 적어지면 당연히 모델의 성능 저하로 이어지게 된다.\n4-2. 모델 학습 관련 문제 우리는 모델을 학습하고 성능을 평가한다. 우리는 모델이 학습하지 않은 데이터에 대해서도 성능이 좋게 나오길 원한다. 바로 일반화(Generalization) 성능이다.\n일반화 성능이 좋다는 것은 모델이 데이터에서 일반적인 패턴과 규칙을 찾아내고 학습해서 새로운 데이터에 대해서도 예측을 잘한다는 것이다. 이것이 머신러닝에서 우리의 궁극적인 목표가 된다.\n하지만 모델이 학습 데이터에서만 너무 잘 맞춰져서 새로운 데이터를 예측을 잘 하지 못하고 성능이 저하되는 현상이 발생한다. Overfitting이 발생하는 것이다. Overfitting은 일반화 성능이 부족한 것이다.\n때로는 모델이 학습 데이터조차 잘 예측하지 못하는 상황이 발생한다. Underfitting을 말하는 것이다. Underfitting은 모델이 너무 단순해서 데이터의 복잡한 패턴을 못 잡는 상황이다.\nhyperparameter에 따른 문제도 있다. hyperparameter는 사람이 직접 조작할 수 있는 parameter를 말한다. learning rate, batch size, optimizer 등 수많은 설정이 성능에 민감하게 작용한다. 물론 이와 관련해서 적절한 hyperparameter를 찾아주는 알고리즘이 많이 나와있지만 많은 알고리즘 속에서도 어떤 알고리즘을 선택할 것인지는 다시 우리의 문제가 된다.\n이런 문제들이 발생하기 전에 애초부터 모델 선택에서 문제가 발생할 수 있다. Tree, SVM, CNN, Transformer 등\u0026hellip; 이 각각에서도 파생되고 생성되는 모델들이 엄청나게 많고 매년 개발되고 연구되고 있다. 본인도 Kaggle, Dacon 등과 같은 AI Competition 사이트에서 대회들을 해보면서 느낀 것은 데이터를 분석하고 전처리를 하고 나서 이런 모델을 사용하면 좋지 않을까?하고 항상 예측을 먼저 해보고 모델을 선택한다. 하지만 예상과는 달리 성능이 안좋을 때가 많다. 어떤 모델에서는 성능이 좋은데 어떤 모델에서는 또 성능이 안좋게 나온다. Decision Tree나 Linear Regression같은 복잡하지 않고 해석이 쉬운 모델들은 우리가 확인하고 왜 성능이 안좋게 나오는지 예상하고 분석할 수 있다. 하지만 Random Forest, Neural Networks, Transformer처럼 복잡하고 최신의 모델로 갈수록 점차 사람이 해석하기 어려워지고 흔히 말하는 black box로 왜 이런 결과가 나오고 왜 성능이 안좋게 나오는 지 알기 힘들게 된다.\n따라서 적절한 모델의 선택은 매우 중요하고 문제의 절반을 차지한다고 생각된다.\n4-3. 계산 자원 문제 마지막으로 자원의 대한 문제가 있다. GPT와 같은 최신의 LLM은 한 번 학습하는 데 몇 천만원이 들고 몇개월이 걸린다고 한다. 모델의 크기와 데이터의 크기가 커질수록 고성능 GPU, TPU, 대용량 메모리가 필요하다.\n대회를 하면서 느낀 점 중에서 또 하나는 확실히 컴퓨팅 파워가 많이 필요하다는 것이다. 먼저 대회에서 주어지는 문제와 데이터를 확인하고 분석한다. 그리고 어떤 기법을 사용할 지 어떤 모델을 사용할 지 시작하기 전에 생각을 하고 코드를 작성한다. 내 설계가 한 번에 좋은 결과를 가져오면 좋겠지만, 그렇지 않은 경우가 태반이다. 모델의 일반화 성능을 끌어올리기 위해 수많은 시행착오를 겪고 수정하고 반복을 하면서 실험해봐야 어떤 접근과 기법이 좋은지 안 좋은지 알 수 있다. 그렇다면 당연히 고성능 GPU, CPU를 가진 사람들이 더 많이 시행하고 결과를 제출할 것이다. 물론 컴퓨팅 성능이 좋다고해서 무조건 좋은 성과를 내는 것은 아니고 보장하는 것도 아니지만, 더 많은 시도와 실험이 가능해질수록 결과의 차이는 점점 커질 수 있다.\n5. 결론 최근에는 거대한 모델들의 시간과 비용을 줄이기 위해 많은 최적화 방법들이 연구되고 있다. quantization, LoRA, distillation. Pruning, MoE 등\u0026hellip; 이외에도 엄청나게 많다. 따라서 이런 방법들을 우리가 판단하여 적절히 활용하고 적용해야한다. 무조건 모델의 크기가 크고 복잡하다고 해서 좋은 것이 아니다. 물론 현실의 문제는 복잡하고 현실 세계의 문제를 해결하기 위해 점점 모델이 복잡해지고 거대해지고 있지만, 중요한 것은 내가 해결하고자 하는 문제가 어떤 문제인지 파악하고 문제에 맞는 모델을 선택하고 설계하고 찾아 나가는 것이 중요하다. 뭐든지 적절한 것이 있고 중간이 있는 법이다.\n","permalink":"https://xooyong.github.io/posts/2025-06/2025-06-24-about_machine_learning/","summary":"\u003ch2 id=\"1-machine-learning\"\u003e1. Machine Learning\u003c/h2\u003e\n\u003cp\u003eMachine Learning이란 뭘까? 1959년, 아서 사무엘은 ML을 이렇게 정의했다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;기계가 일일이 코드로 명시하지 않은 동작을 데이터로부터 학습하여 실핼할 수 있도록 하는 알고리즘을 개발하는 연구 분야이다.\u0026rdquo;\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e쉽게 말하면 기계가 스스로 학습하는 것이라고 할 수 있겠다. 그렇다면 기계는 어떤 것을 학습하고 어떻게 학습할 수 있을까? 먼저 학습을 하기 위해서 어떤 요소들이 필요한지 살펴보자.\n나는 기계가 학습하기 위해서, 크게 3가지 요소로 나누어 진다고 생각한다.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eData\u003c/li\u003e\n\u003cli\u003eML 개념 - Supervised Learning, Unsupervised Learning, Reinforcement Learning\u0026hellip;\u003c/li\u003e\n\u003cli\u003eAlgorithm - Linear Regression, Tree, Graph, Neural Networks\u0026hellip;\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e모델은 \u003cstrong\u003e데이터\u003c/strong\u003e를 기반으로 학습한다. 데이터가 없다면 모델은 아무것도 할 수가 없다. 데이터의 품질, 양에 따라서 모델의 성능이 크게 좌우된다. 많은 양의 데이터도 중요하지만 데이터의 품질이 중요하다고 생각된다. 데이터가 품질이 좋고 해당되는 분야에 대해서 잘 표현되어 있다면 모델은 이를 통해 좋은 정보와 특징들을 학습하고 우리에게 원하는 답을 찾아 줄 것이다.\u003c/p\u003e","title":"About Machine Learning"},{"content":"","permalink":"https://xooyong.github.io/posts/2025-06/first/","summary":"","title":"My 1st post"}]