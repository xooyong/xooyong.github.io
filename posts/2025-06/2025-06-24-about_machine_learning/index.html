<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>About Machine Learning | xooyong blog</title><meta name=keywords content="ML"><meta name=description content="ML에 대한 나의 생각"><meta name=author content="Me"><link rel=canonical href=https://xooyong.github.io/posts/2025-06/2025-06-24-about_machine_learning/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://xooyong.github.io/posts/2025-06/2025-06-24-about_machine_learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://xooyong.github.io/posts/2025-06/2025-06-24-about_machine_learning/"><meta property="og:site_name" content="xooyong blog"><meta property="og:title" content="About Machine Learning"><meta property="og:description" content="ML에 대한 나의 생각"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-24T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-24T00:00:00+00:00"><meta property="article:tag" content="ML"><meta property="og:image" content="https://xooyong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://xooyong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="About Machine Learning"><meta name=twitter:description content="ML에 대한 나의 생각"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://xooyong.github.io/posts/"},{"@type":"ListItem","position":2,"name":"About Machine Learning","item":"https://xooyong.github.io/posts/2025-06/2025-06-24-about_machine_learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"About Machine Learning","name":"About Machine Learning","description":"ML에 대한 나의 생각","keywords":["ML"],"articleBody":"1. Machine Learning Machine Learning이란 뭘까? 1959년, 아서 사무엘은 ML을 이렇게 정의했다.\n“기계가 일일이 코드로 명시하지 않은 동작을 데이터로부터 학습하여 실핼할 수 있도록 하는 알고리즘을 개발하는 연구 분야이다.”\n쉽게 말하면 기계가 스스로 학습하는 것이라고 할 수 있겠다. 그렇다면 기계는 어떤 것을 학습하고 어떻게 학습할 수 있을까? 먼저 학습을 하기 위해서 어떤 요소들이 필요한지 살펴보자. 나는 기계가 학습하기 위해서, 크게 3가지 요소로 나누어 진다고 생각한다.\nData ML 개념 - Supervised Learning, Unsupervised Learning, Reinforcement Learning… Algorithm - Linear Regression, Tree, Graph, Neural Networks… 모델은 데이터를 기반으로 학습한다. 데이터가 없다면 모델은 아무것도 할 수가 없다. 데이터의 품질, 양에 따라서 모델의 성능이 크게 좌우된다. 많은 양의 데이터도 중요하지만 데이터의 품질이 중요하다고 생각된다. 데이터가 품질이 좋고 해당되는 분야에 대해서 잘 표현되어 있다면 모델은 이를 통해 좋은 정보와 특징들을 학습하고 우리에게 원하는 답을 찾아 줄 것이다.\nML 개념은 ML에서 가장 중요한 요소이다. 모델의 학습 방식이라고도 할 수 있지만 데이터의 형태라고도 할 수 있다. Supervised learning에서는 Classification, Regression으로 나누어 지고, Unsupervised learning에서는 Clustering, Anomaly Detection 등으로 나누어 진다. 이렇게 데이터의 형태에 따라서 ML을 하는데 있어서 아키텍쳐, 방법, 구성 등 모든 것이 바뀔 수 있다.\n데이터가 주어졌고 데이터의 형태를 우리가 알았다. 이제 마지막으로 Algorithm이 필요하다. Algorithm은 모델이라고도 할 수 있다. 모델은 데이터를 입력받고 예측값을 출력한다. 이 과정 속에서 모델은 지속적으로 학습하고 틀린 문제를 보완할 것이다. 즉 parameter를 업데이트하는 절차를 따른다.\n자 이렇게 ML을 하기 위한 모든 요소들이 갖추어졌다. 그렇다면 Machine Learning에서 Machine은 무엇을 배운다는 것일까? 가장 대표적인 예를 살펴보자\n$$ y=f(x) $$\nML은 입력 데이터 x로부터 출력 y를 잘 예측하는 함수 f를 찾는 과정이라고 할 수 있다. 여기서 머신러닝의 목적은 궁극적으로 f를 찾는 것이다. 데이터셋 $(x_1, y_1), (x_2, y_2), …, (x_n, y_n)$으로부터 f의 형태와 파라미터를 찾아서, 새로운 x에 대해 y를 잘 예측하는 모델을 만드는 것이다. Linear Regresssion에서는 $f(x)=wx+b$, Neural Network에서는 $f(x)$가 여러 층의 비선형 함수 조합, Decision Tree에서는 $f(x)$가 조건문 분기 구조가 될 것이다.\n그렇지만 이 $y=f(x)$라는 해석은 기본적으로 Supervised Learning에 해당하는 해석이다. 그리고 Supervised Learning은 ML에서의 핵심 학습 방식이다.\n2. Supervised Learning Supervised Learning의 대표적인 정의를 보자.\nSupervised learning is the process of trying to infer from labeled data the underlying function that produced the labels associated with the data\nSupervised learning은 레이블이 있는 데이터로부터 해당 레이블을 생성한 근본적인 함수를 추론하려는 과정이다.\nlabeled data라는 것은 뭘까? 정답이 있는 데이터를 말하는 것이고 label은 정답이다. 그리고 근본적인 함수는 f를 말하는 것이다. 위에서 말했던 내용과 같다. 결국 labeled data $D={(x_1, y_1), (x_2, y_2), … ,(x_n, y_n)}$에 대해서 입력 $x$를 받아 출력 $y$(label)을 예측하는 근본적인 함수 f를 학습하고 가장 $y$(label)을 잘 표현하는 f를 찾는 것이다.\n처음에서도 말했듯이 Supervised learning은 크게 Classification, Regression으로 나뉜다. 공통점은 입력 데이터 $x$와 정답 $y$가 주어진 상황에서, 입력으로부터 정답을 예측하는 함수 $f(x)$를 학습하는 것을 목표로 한다.\nClassification은 입력 데이터를 미리 정의된 **클래스(범주)**중 하나로 분류하는 문제이다. 정답 $y$는 이산형(discrete)값이고 예를 들어, 개, 고양이, 양성/음성, 숫자0~9로 표현된다. 문제의 정답은 이렇게 표현되지만 모델이 이를 학습하고 나오는 output은 확률이다. 그러니까 output이 0.5이상이면 개라고 분류하고 0.5 미만이면 고양이라고 분류할 것이다. 확률에 따라서 관측치의 범주를 예측하고 분류한다.\nRegression은 입력 데이터를 기반으로 연속적인 수치값을 예측하는 문제이다. 정답 $y$는 연속형(continuous)값이고 예를 들어, 가격, 온도, 수치 등을 말한다. output은 보통 실수(float)값이다.\nML을 하기 위해서는 어떤 것들이 필요하고 어떤 것을 학습하는지도 알아보았다. 모델은 한 번에 y를 잘 표현하는 f를 바로 찾아내지는 못할것이다. 반복적으로 학습을 하고 예측을 하면서 보완해나간다. 모델의 예측이 정답과 틀렸을 때 어떻게 보완하고 틀렸다는 것을 알 수 있을까? 또는 어떻게 모델의 성능이 좋고 나쁘다는 것을 평가할 수 있을까?\n3. 모델의 성능 평가 기준 먼저 Optimization의 개념을 알아야 한다. Optimization은 어떤 **목적함수(objective function)**의 함수값을 Optimization(maximize 또는 minimize)시키는 parameter(변수)조합을 찾는 문제를 말한다. 여기서 objective function의 구조 및 형태와 이에 따른 찾아야 될 parameter 형태에 따라 다양한 방법이 존재한다. 또한 다루고자 하는 Optimization 문제는 별도의 제약조건이 있는 경우와 없는 경우가 존재한다.\n$$\\underset{x}{minimize} \\quad f(x)$$ subject to $$g_i(x)\u003c=0, i=1,…,m$$ $$h_j(x)=0, j=1,…,p$$\n(where $f:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ is the objective function to be minimized over the $n$-variable vector $x$)\nOptimization 문제는 크게 maximization 문제와 minimization 문제로 나눌 수 있는데 objective function이 이윤, 점수(score) 등인 경우에는 maximization 문제가 되고, 비용(cost), 손실(loss), 에러(error) 등인 경우에는 minimization 문제가 된다.\n일반적으로 ML 모델들은 앞서 설명한 기준 함수를 Objective, Cost, Loss Function으로 정의하고 이 기준 함수를 minimize 또는 maximize 하는 방향으로 모델을 학습시킨다.\n4. 우리의 문제 이렇게 ML에 대해서 간단하게 알아보았다. 우리가 어떤 문제를 해결하고 싶어서 데이터를 수집하고 데이터의 형태를 알아보고 분석하고 이에 맞는 모델을 선택하고 어떻게 평가할 지, 어떻게 최적화할 지 선택하고 정하게 될 것이다. 이 과정 속에서는 우리가 항상 생각하고 고려해야되는 문제들이 있다.\n4-1. 데이터의 문제 데이터는 거짓말을 하지 않는다… 하지만 현실 세계에서의 데이터는 완벽하거나 정확하지 않고 불확실하고 잘못되거나 지저분한 경우가 많다. 또는 label이 없는 경우가 많다. 모델은 데이터가 없으면 아무것도 할 수 없고 데이터의 품질이 좋으면 자연스럽게 모델의 성능도 높아질 것이다. 모델의 성공 여부는 데이터 품질에 크게 좌우된다.\n먼저, 우리는 품질 좋은 데이터를 얻기 위해, 모델의 성능을 높이기 위해 데이터를 가공하고 정제해야 한다. 데이터 내부에 noise, missing value, 잘못된 labeling 등 이러한 문제들을 해결해야 한다. 데이터의 noise는 관련성이 없거나 무의미한 데이터, 무작위 오류 또는 기본 구조와 추출하고자 하는 진실을 왜곡하는 편차(bias)를 의미한다. missing value(결측치)는 데이터에서 값이 비어있거나 누락된 부분을 말한다. 잘못된 label은 사람의 실수로 인해 발생할 수 있다. 예를 들어 개 이미지가 고양이로 잘못 라벨링되는 등 다양한 방식으로 발생할 수 있다.\n데이터에 편향이 존재할 수도 있다. 특정 인종, 성별, 지역 등으로 인해 불균형한 분포를 이루고 이에 따라 class imbalance가 발생하고 학습된 모델이 차별적으로 예측하거나 어떤 class에 대해서는 예측을 잘 못할 수도 있다. 예를 들면, 개와 고양이 사진이 있는데 개에 대한 사진만 너무 많아서 고양이 대해서는 잘 예측하지 못하는 것이다. 또는 고양이도 개라고 예측할 수도 있다.\n데이터 자체가 부족할 수도 있다. 기업 관점에서 자기들이 서비스하는 부분에서 발생하는 데이터는 귀중한 자산이 되고 차별점이 될 것이다. 이에 따라 우리는 데이터 확보가 어려울 수 있고 또는 의료, 법률 등 민감 분야에서 labeled data 확보가 어렵다. 이렇게 되면 작은 데이터셋은 Overfitting, Underfitting 문제와 직결된다.\n데이터에 label이 없다면 우리가 수동으로 labeling 해줄 수 있다. 하지만 labeling 자체가 시간과 비용이 많이 들고 해당 데이터의 도메인 전문가가 필요할 것이다. 수동 라벨링에는 편향성 위험이 발생한다. 이는 labeler가 무의식적으로 한 class를 다른 class보다 선호할 수 있기 때문이다.\n데이터가 복잡한 형태를 띄고 있을수도 있다. The curse of dimensionality는 데이터의 feature 개수가 관측치의 개수보다 많아져 즉, 차원이 증가할 수록 개별 차원 내 학습할 데이터 수가 적어지는(sparse)현상이 발생한다. 낮은 차원에서 촘촘하게 분포하고 있던 데이터가 차원이 높아질수록 각각의 관측치 사이의 거리가 점점 멀어지게 되고 그 사이에 빈 공간이 생긴다. 컴퓨터 상으로 0으로 채워졌다는 뜻이다. 정보가 없어지고 정보가 적어지면 당연히 모델의 성능 저하로 이어지게 된다.\n4-2. 모델 학습 관련 문제 우리는 모델을 학습하고 성능을 평가한다. 우리는 모델이 학습하지 않은 데이터에 대해서도 성능이 좋게 나오길 원한다. 바로 일반화(Generalization) 성능이다.\n일반화 성능이 좋다는 것은 모델이 데이터에서 일반적인 패턴과 규칙을 찾아내고 학습해서 새로운 데이터에 대해서도 예측을 잘한다는 것이다. 이것이 머신러닝에서 우리의 궁극적인 목표가 된다.\n하지만 모델이 학습 데이터에서만 너무 잘 맞춰져서 새로운 데이터를 예측을 잘 하지 못하고 성능이 저하되는 현상이 발생한다. Overfitting이 발생하는 것이다. Overfitting은 일반화 성능이 부족한 것이다.\n때로는 모델이 학습 데이터조차 잘 예측하지 못하는 상황이 발생한다. Underfitting을 말하는 것이다. Underfitting은 모델이 너무 단순해서 데이터의 복잡한 패턴을 못 잡는 상황이다.\nhyperparameter에 따른 문제도 있다. hyperparameter는 사람이 직접 조작할 수 있는 parameter를 말한다. learning rate, batch size, optimizer 등 수많은 설정이 성능에 민감하게 작용한다. 물론 이와 관련해서 적절한 hyperparameter를 찾아주는 알고리즘이 많이 나와있지만 많은 알고리즘 속에서도 어떤 알고리즘을 선택할 것인지는 다시 우리의 문제가 된다.\n이런 문제들이 발생하기 전에 애초부터 모델 선택에서 문제가 발생할 수 있다. Tree, SVM, CNN, Transformer 등… 이 각각에서도 파생되고 생성되는 모델들이 엄청나게 많고 매년 개발되고 연구되고 있다. 본인도 Kaggle, Dacon 등과 같은 AI Competition 사이트에서 대회들을 해보면서 느낀 것은 데이터를 분석하고 전처리를 하고 나서 이런 모델을 사용하면 좋지 않을까?하고 항상 예측을 먼저 해보고 모델을 선택한다. 하지만 예상과는 달리 성능이 안좋을 때가 많다. 어떤 모델에서는 성능이 좋은데 어떤 모델에서는 또 성능이 안좋게 나온다. Decision Tree나 Linear Regression같은 복잡하지 않고 해석이 쉬운 모델들은 우리가 확인하고 왜 성능이 안좋게 나오는지 예상하고 분석할 수 있다. 하지만 Random Forest, Neural Networks, Transformer처럼 복잡하고 최신의 모델로 갈수록 점차 사람이 해석하기 어려워지고 흔히 말하는 black box로 왜 이런 결과가 나오고 왜 성능이 안좋게 나오는 지 알기 힘들게 된다.\n따라서 적절한 모델의 선택은 매우 중요하고 문제의 절반을 차지한다고 생각된다.\n4-3. 계산 자원 문제 마지막으로 자원의 대한 문제가 있다. GPT와 같은 최신의 LLM은 한 번 학습하는 데 몇 천만원이 들고 몇개월이 걸린다고 한다. 모델의 크기와 데이터의 크기가 커질수록 고성능 GPU, TPU, 대용량 메모리가 필요하다.\n대회를 하면서 느낀 점 중에서 또 하나는 확실히 컴퓨팅 파워가 많이 필요하다는 것이다. 먼저 대회에서 주어지는 문제와 데이터를 확인하고 분석한다. 그리고 어떤 기법을 사용할 지 어떤 모델을 사용할 지 시작하기 전에 생각을 하고 코드를 작성한다. 내 설계가 한 번에 좋은 결과를 가져오면 좋겠지만, 그렇지 않은 경우가 태반이다. 모델의 일반화 성능을 끌어올리기 위해 수많은 시행착오를 겪고 수정하고 반복을 하면서 실험해봐야 어떤 접근과 기법이 좋은지 안 좋은지 알 수 있다. 그렇다면 당연히 고성능 GPU, CPU를 가진 사람들이 더 많이 시행하고 결과를 제출할 것이다. 물론 컴퓨팅 성능이 좋다고해서 무조건 좋은 성과를 내는 것은 아니고 보장하는 것도 아니지만, 더 많은 시도와 실험이 가능해질수록 결과의 차이는 점점 커질 수 있다.\n5. 결론 최근에는 거대한 모델들의 시간과 비용을 줄이기 위해 많은 최적화 방법들이 연구되고 있다. quantization, LoRA, distillation. Pruning, MoE 등… 이외에도 엄청나게 많다. 따라서 이런 방법들을 우리가 판단하여 적절히 활용하고 적용해야한다. 무조건 모델의 크기가 크고 복잡하다고 해서 좋은 것이 아니다. 물론 현실의 문제는 복잡하고 현실 세계의 문제를 해결하기 위해 점점 모델이 복잡해지고 거대해지고 있지만, 중요한 것은 내가 해결하고자 하는 문제가 어떤 문제인지 파악하고 문제에 맞는 모델을 선택하고 설계하고 찾아 나가는 것이 중요하다. 뭐든지 적절한 것이 있고 중간이 있는 법이다.\n","wordCount":"1513","inLanguage":"en","image":"https://xooyong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-06-24T00:00:00Z","dateModified":"2025-06-24T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xooyong.github.io/posts/2025-06/2025-06-24-about_machine_learning/"},"publisher":{"@type":"Organization","name":"xooyong blog","logo":{"@type":"ImageObject","url":"https://xooyong.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xooyong.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://xooyong.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://xooyong.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://xooyong.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://xooyong.github.io/archives/ title=archives><span>archives</span></a></li><li><a href=https://xooyong.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://xooyong.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://xooyong.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">About Machine Learning</h1><div class=post-description>ML에 대한 나의 생각</div><div class=post-meta><span title='2025-06-24 00:00:00 +0000 UTC'>June 24, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1513 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/2025-06/2025-06-24-About_Machine_Learning.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=post-content><h2 id=1-machine-learning>1. Machine Learning<a hidden class=anchor aria-hidden=true href=#1-machine-learning>#</a></h2><p>Machine Learning이란 뭘까? 1959년, 아서 사무엘은 ML을 이렇게 정의했다.</p><blockquote><p>&ldquo;기계가 일일이 코드로 명시하지 않은 동작을 데이터로부터 학습하여 실핼할 수 있도록 하는 알고리즘을 개발하는 연구 분야이다.&rdquo;</p></blockquote><p>쉽게 말하면 기계가 스스로 학습하는 것이라고 할 수 있겠다. 그렇다면 기계는 어떤 것을 학습하고 어떻게 학습할 수 있을까? 먼저 학습을 하기 위해서 어떤 요소들이 필요한지 살펴보자.
나는 기계가 학습하기 위해서, 크게 3가지 요소로 나누어 진다고 생각한다.</p><ol><li>Data</li><li>ML 개념 - Supervised Learning, Unsupervised Learning, Reinforcement Learning&mldr;</li><li>Algorithm - Linear Regression, Tree, Graph, Neural Networks&mldr;</li></ol><p>모델은 <strong>데이터</strong>를 기반으로 학습한다. 데이터가 없다면 모델은 아무것도 할 수가 없다. 데이터의 품질, 양에 따라서 모델의 성능이 크게 좌우된다. 많은 양의 데이터도 중요하지만 데이터의 품질이 중요하다고 생각된다. 데이터가 품질이 좋고 해당되는 분야에 대해서 잘 표현되어 있다면 모델은 이를 통해 좋은 정보와 특징들을 학습하고 우리에게 원하는 답을 찾아 줄 것이다.</p><p><strong>ML 개념</strong>은 ML에서 가장 중요한 요소이다. 모델의 학습 방식이라고도 할 수 있지만 데이터의 형태라고도 할 수 있다. Supervised learning에서는 Classification, Regression으로 나누어 지고, Unsupervised learning에서는 Clustering, Anomaly Detection 등으로 나누어 진다. 이렇게 데이터의 형태에 따라서 ML을 하는데 있어서 아키텍쳐, 방법, 구성 등 모든 것이 바뀔 수 있다.</p><p>데이터가 주어졌고 데이터의 형태를 우리가 알았다. 이제 마지막으로 <strong>Algorithm</strong>이 필요하다. Algorithm은 모델이라고도 할 수 있다. 모델은 데이터를 입력받고 예측값을 출력한다. 이 과정 속에서 모델은 지속적으로 학습하고 틀린 문제를 보완할 것이다. 즉 parameter를 업데이트하는 절차를 따른다.</p><p>자 이렇게 ML을 하기 위한 모든 요소들이 갖추어졌다. 그렇다면 Machine Learning에서 Machine은 무엇을 배운다는 것일까? 가장 대표적인 예를 살펴보자</p><p>$$
y=f(x)
$$</p><p>ML은 입력 데이터 x로부터 출력 y를 잘 예측하는 함수 <strong>f</strong>를 찾는 과정이라고 할 수 있다. 여기서 머신러닝의 목적은 궁극적으로 <strong>f</strong>를 찾는 것이다. 데이터셋 $(x_1, y_1), (x_2, y_2), &mldr;, (x_n, y_n)$으로부터 f의 형태와 파라미터를 찾아서, 새로운 x에 대해 y를 잘 예측하는 모델을 만드는 것이다. Linear Regresssion에서는 $f(x)=wx+b$, Neural Network에서는 $f(x)$가 여러 층의 비선형 함수 조합, Decision Tree에서는 $f(x)$가 조건문 분기 구조가 될 것이다.</p><p>그렇지만 이 $y=f(x)$라는 해석은 기본적으로 Supervised Learning에 해당하는 해석이다. 그리고 Supervised Learning은 ML에서의 핵심 학습 방식이다.</p><h2 id=2-supervised-learning>2. Supervised Learning<a hidden class=anchor aria-hidden=true href=#2-supervised-learning>#</a></h2><p>Supervised Learning의 대표적인 정의를 보자.</p><blockquote><p>Supervised learning is the process of trying to infer from labeled data the underlying function that produced the labels associated with the data</p></blockquote><blockquote><p>Supervised learning은 레이블이 있는 데이터로부터 해당 레이블을 생성한 근본적인 함수를 추론하려는 과정이다.</p></blockquote><p><strong>labeled data</strong>라는 것은 뭘까? 정답이 있는 데이터를 말하는 것이고 label은 정답이다. 그리고 근본적인 함수는 f를 말하는 것이다. 위에서 말했던 내용과 같다. 결국 labeled data $D={(x_1, y_1), (x_2, y_2), &mldr; ,(x_n, y_n)}$에 대해서 입력 $x$를 받아 출력 $y$(label)을 예측하는 근본적인 함수 f를 학습하고 가장 $y$(label)을 잘 표현하는 f를 찾는 것이다.</p><p>처음에서도 말했듯이 Supervised learning은 크게 Classification, Regression으로 나뉜다. 공통점은 입력 데이터 $x$와 정답 $y$가 주어진 상황에서, 입력으로부터 정답을 예측하는 함수 $f(x)$를 학습하는 것을 목표로 한다.</p><p><strong>Classification</strong>은 입력 데이터를 미리 정의된 **클래스(범주)**중 하나로 분류하는 문제이다. 정답 $y$는 이산형(discrete)값이고 예를 들어, 개, 고양이, 양성/음성, 숫자0~9로 표현된다. 문제의 정답은 이렇게 표현되지만 모델이 이를 학습하고 나오는 output은 확률이다. 그러니까 output이 0.5이상이면 개라고 분류하고 0.5 미만이면 고양이라고 분류할 것이다. 확률에 따라서 관측치의 범주를 예측하고 분류한다.</p><p><strong>Regression</strong>은 입력 데이터를 기반으로 <strong>연속적인 수치값</strong>을 예측하는 문제이다. 정답 $y$는 연속형(continuous)값이고 예를 들어, 가격, 온도, 수치 등을 말한다. output은 보통 실수(float)값이다.</p><p>ML을 하기 위해서는 어떤 것들이 필요하고 어떤 것을 학습하는지도 알아보았다. 모델은 한 번에 y를 잘 표현하는 f를 바로 찾아내지는 못할것이다. 반복적으로 학습을 하고 예측을 하면서 보완해나간다. 모델의 예측이 정답과 틀렸을 때 어떻게 보완하고 틀렸다는 것을 알 수 있을까? 또는 어떻게 모델의 성능이 좋고 나쁘다는 것을 평가할 수 있을까?</p><h2 id=3-모델의-성능-평가-기준>3. 모델의 성능 평가 기준<a hidden class=anchor aria-hidden=true href=#3-모델의-성능-평가-기준>#</a></h2><p>먼저 <strong>Optimization</strong>의 개념을 알아야 한다. Optimization은 어떤 **목적함수(objective function)**의 함수값을 Optimization(maximize 또는 minimize)시키는 <strong>parameter(변수)조합</strong>을 찾는 문제를 말한다. 여기서 objective function의 구조 및 형태와 이에 따른 찾아야 될 parameter 형태에 따라 다양한 방법이 존재한다. 또한 다루고자 하는 Optimization 문제는 별도의 제약조건이 있는 경우와 없는 경우가 존재한다.</p><p>$$\underset{x}{minimize} \quad f(x)$$
subject to $$g_i(x)&lt;=0, i=1,&mldr;,m$$
$$h_j(x)=0, j=1,&mldr;,p$$</p><p>(where $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is the objective function to be minimized over the $n$-variable vector $x$)</p><p>Optimization 문제는 크게 <strong>maximization</strong> 문제와 <strong>minimization</strong> 문제로 나눌 수 있는데 objective function이 이윤, 점수(score) 등인 경우에는 maximization 문제가 되고, 비용(cost), 손실(loss), 에러(error) 등인 경우에는 minimization 문제가 된다.</p><p>일반적으로 ML 모델들은 앞서 설명한 기준 함수를 Objective, Cost, Loss Function으로 정의하고 이 기준 함수를 minimize 또는 maximize 하는 방향으로 모델을 학습시킨다.</p><h2 id=4-우리의-문제>4. 우리의 문제<a hidden class=anchor aria-hidden=true href=#4-우리의-문제>#</a></h2><p>이렇게 ML에 대해서 간단하게 알아보았다. 우리가 어떤 문제를 해결하고 싶어서 데이터를 수집하고 데이터의 형태를 알아보고 분석하고 이에 맞는 모델을 선택하고 어떻게 평가할 지, 어떻게 최적화할 지 선택하고 정하게 될 것이다. 이 과정 속에서는 우리가 항상 생각하고 고려해야되는 문제들이 있다.</p><h3 id=4-1-데이터의-문제>4-1. 데이터의 문제<a hidden class=anchor aria-hidden=true href=#4-1-데이터의-문제>#</a></h3><p>데이터는 거짓말을 하지 않는다&mldr; 하지만 현실 세계에서의 데이터는 완벽하거나 정확하지 않고 불확실하고 잘못되거나 지저분한 경우가 많다. 또는 label이 없는 경우가 많다. 모델은 데이터가 없으면 아무것도 할 수 없고 데이터의 품질이 좋으면 자연스럽게 모델의 성능도 높아질 것이다. 모델의 성공 여부는 데이터 품질에 크게 좌우된다.</p><p>먼저, 우리는 품질 좋은 데이터를 얻기 위해, 모델의 성능을 높이기 위해 데이터를 <strong>가공</strong>하고 <strong>정제</strong>해야 한다. 데이터 내부에 noise, missing value, 잘못된 labeling 등 이러한 문제들을 해결해야 한다. 데이터의 noise는 관련성이 없거나 무의미한 데이터, 무작위 오류 또는 기본 구조와 추출하고자 하는 진실을 왜곡하는 편차(bias)를 의미한다. missing value(결측치)는 데이터에서 값이 비어있거나 누락된 부분을 말한다. 잘못된 label은 사람의 실수로 인해 발생할 수 있다. 예를 들어 개 이미지가 고양이로 잘못 라벨링되는 등 다양한 방식으로 발생할 수 있다.</p><p>데이터에 <strong>편향</strong>이 존재할 수도 있다. 특정 인종, 성별, 지역 등으로 인해 불균형한 분포를 이루고 이에 따라 class imbalance가 발생하고 학습된 모델이 차별적으로 예측하거나 어떤 class에 대해서는 예측을 잘 못할 수도 있다. 예를 들면, 개와 고양이 사진이 있는데 개에 대한 사진만 너무 많아서 고양이 대해서는 잘 예측하지 못하는 것이다. 또는 고양이도 개라고 예측할 수도 있다.</p><p>데이터 자체가 <strong>부족</strong>할 수도 있다. 기업 관점에서 자기들이 서비스하는 부분에서 발생하는 데이터는 귀중한 자산이 되고 차별점이 될 것이다. 이에 따라 우리는 데이터 확보가 어려울 수 있고 또는 의료, 법률 등 민감 분야에서 labeled data 확보가 어렵다. 이렇게 되면 작은 데이터셋은 Overfitting, Underfitting 문제와 직결된다.</p><p>데이터에 label이 없다면 우리가 수동으로 labeling 해줄 수 있다. 하지만 labeling 자체가 시간과 비용이 많이 들고 해당 데이터의 도메인 전문가가 필요할 것이다. 수동 라벨링에는 편향성 위험이 발생한다. 이는 labeler가 무의식적으로 한 class를 다른 class보다 선호할 수 있기 때문이다.</p><p>데이터가 복잡한 형태를 띄고 있을수도 있다. The curse of dimensionality는 데이터의 feature 개수가 관측치의 개수보다 많아져 즉, 차원이 증가할 수록 개별 차원 내 학습할 데이터 수가 적어지는(sparse)현상이 발생한다.
낮은 차원에서 촘촘하게 분포하고 있던 데이터가 차원이 높아질수록 각각의 관측치 사이의 거리가 점점 멀어지게 되고 그 사이에 빈 공간이 생긴다. 컴퓨터 상으로 0으로 채워졌다는 뜻이다. 정보가 없어지고 정보가 적어지면 당연히 모델의 성능 저하로 이어지게 된다.</p><h3 id=4-2-모델-학습-관련-문제>4-2. 모델 학습 관련 문제<a hidden class=anchor aria-hidden=true href=#4-2-모델-학습-관련-문제>#</a></h3><p>우리는 모델을 학습하고 성능을 평가한다. 우리는 모델이 학습하지 않은 데이터에 대해서도 성능이 좋게 나오길 원한다. 바로 <strong>일반화(Generalization)</strong> 성능이다.</p><p>일반화 성능이 좋다는 것은 모델이 데이터에서 일반적인 패턴과 규칙을 찾아내고 학습해서 새로운 데이터에 대해서도 예측을 잘한다는 것이다. 이것이 머신러닝에서 우리의 궁극적인 목표가 된다.</p><p>하지만 모델이 학습 데이터에서만 너무 잘 맞춰져서 새로운 데이터를 예측을 잘 하지 못하고 성능이 저하되는 현상이 발생한다. <strong>Overfitting</strong>이 발생하는 것이다. Overfitting은 일반화 성능이 부족한 것이다.</p><p>때로는 모델이 학습 데이터조차 잘 예측하지 못하는 상황이 발생한다. <strong>Underfitting</strong>을 말하는 것이다. Underfitting은 모델이 너무 단순해서 데이터의 복잡한 패턴을 못 잡는 상황이다.</p><p><strong>hyperparameter</strong>에 따른 문제도 있다. hyperparameter는 사람이 직접 조작할 수 있는 parameter를 말한다. learning rate, batch size, optimizer 등 수많은 설정이 성능에 민감하게 작용한다. 물론 이와 관련해서 적절한 hyperparameter를 찾아주는 알고리즘이 많이 나와있지만 많은 알고리즘 속에서도 어떤 알고리즘을 선택할 것인지는 다시 우리의 문제가 된다.</p><p>이런 문제들이 발생하기 전에 애초부터 모델 선택에서 문제가 발생할 수 있다. Tree, SVM, CNN, Transformer 등&mldr; 이 각각에서도 파생되고 생성되는 모델들이 엄청나게 많고 매년 개발되고 연구되고 있다. 본인도 Kaggle, Dacon 등과 같은 AI Competition 사이트에서 대회들을 해보면서 느낀 것은 데이터를 분석하고 전처리를 하고 나서 이런 모델을 사용하면 좋지 않을까?하고 항상 예측을 먼저 해보고 모델을 선택한다. 하지만 예상과는 달리 성능이 안좋을 때가 많다. 어떤 모델에서는 성능이 좋은데 어떤 모델에서는 또 성능이 안좋게 나온다. Decision Tree나 Linear Regression같은 복잡하지 않고 해석이 쉬운 모델들은 우리가 확인하고 왜 성능이 안좋게 나오는지 예상하고 분석할 수 있다. 하지만 Random Forest, Neural Networks, Transformer처럼 복잡하고 최신의 모델로 갈수록 점차 사람이 해석하기 어려워지고 흔히 말하는 black box로 왜 이런 결과가 나오고 왜 성능이 안좋게 나오는 지 알기 힘들게 된다.</p><p>따라서 적절한 모델의 선택은 매우 중요하고 문제의 절반을 차지한다고 생각된다.</p><h3 id=4-3-계산-자원-문제>4-3. 계산 자원 문제<a hidden class=anchor aria-hidden=true href=#4-3-계산-자원-문제>#</a></h3><p>마지막으로 자원의 대한 문제가 있다. GPT와 같은 최신의 LLM은 한 번 학습하는 데 몇 천만원이 들고 몇개월이 걸린다고 한다. 모델의 크기와 데이터의 크기가 커질수록 고성능 GPU, TPU, 대용량 메모리가 필요하다.</p><p>대회를 하면서 느낀 점 중에서 또 하나는 확실히 컴퓨팅 파워가 많이 필요하다는 것이다. 먼저 대회에서 주어지는 문제와 데이터를 확인하고 분석한다. 그리고 어떤 기법을 사용할 지 어떤 모델을 사용할 지 시작하기 전에 생각을 하고 코드를 작성한다. 내 설계가 한 번에 좋은 결과를 가져오면 좋겠지만, 그렇지 않은 경우가 태반이다. 모델의 일반화 성능을 끌어올리기 위해 수많은 시행착오를 겪고 수정하고 반복을 하면서 실험해봐야 어떤 접근과 기법이 좋은지 안 좋은지 알 수 있다. 그렇다면 당연히 고성능 GPU, CPU를 가진 사람들이 더 많이 시행하고 결과를 제출할 것이다. 물론 컴퓨팅 성능이 좋다고해서 무조건 좋은 성과를 내는 것은 아니고 보장하는 것도 아니지만, 더 많은 시도와 실험이 가능해질수록 결과의 차이는 점점 커질 수 있다.</p><h2 id=5-결론>5. 결론<a hidden class=anchor aria-hidden=true href=#5-결론>#</a></h2><p>최근에는 거대한 모델들의 시간과 비용을 줄이기 위해 많은 최적화 방법들이 연구되고 있다. quantization, LoRA, distillation. Pruning, MoE 등&mldr; 이외에도 엄청나게 많다. 따라서 이런 방법들을 우리가 판단하여 적절히 활용하고 적용해야한다. 무조건 모델의 크기가 크고 복잡하다고 해서 좋은 것이 아니다. 물론 현실의 문제는 복잡하고 현실 세계의 문제를 해결하기 위해 점점 모델이 복잡해지고 거대해지고 있지만, 중요한 것은 내가 해결하고자 하는 문제가 어떤 문제인지 파악하고 문제에 맞는 모델을 선택하고 설계하고 찾아 나가는 것이 중요하다. 뭐든지 적절한 것이 있고 중간이 있는 법이다.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://xooyong.github.io/tags/ml/>ML</a></li></ul><nav class=paginav><a class=prev href=https://xooyong.github.io/posts/2025-09/2025-09-07-from-tree-to-forest/><span class=title>« Prev</span><br><span>From Tree to Forest</span>
</a><a class=next href=https://xooyong.github.io/posts/2025-06/first/><span class=title>Next »</span><br><span>My 1st post</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share About Machine Learning on x" href="https://x.com/intent/tweet/?text=About%20Machine%20Learning&amp;url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-06%2f2025-06-24-about_machine_learning%2f&amp;hashtags=ML"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share About Machine Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-06%2f2025-06-24-about_machine_learning%2f&amp;title=About%20Machine%20Learning&amp;summary=About%20Machine%20Learning&amp;source=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-06%2f2025-06-24-about_machine_learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share About Machine Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-06%2f2025-06-24-about_machine_learning%2f&title=About%20Machine%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share About Machine Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-06%2f2025-06-24-about_machine_learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share About Machine Learning on whatsapp" href="https://api.whatsapp.com/send?text=About%20Machine%20Learning%20-%20https%3a%2f%2fxooyong.github.io%2fposts%2f2025-06%2f2025-06-24-about_machine_learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share About Machine Learning on telegram" href="https://telegram.me/share/url?text=About%20Machine%20Learning&amp;url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-06%2f2025-06-24-about_machine_learning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share About Machine Learning on ycombinator" href="https://news.ycombinator.com/submitlink?t=About%20Machine%20Learning&u=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-06%2f2025-06-24-about_machine_learning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://xooyong.github.io/>xooyong blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>