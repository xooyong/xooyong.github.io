<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Decision Tree | xooyong blog</title><meta name=keywords content="ML"><meta name=description content="Decision Tree를 밑바닥부터 차근차근 알아보자..."><meta name=author content="Me"><link rel=canonical href=https://xooyong.github.io/posts/2025-07/2025-07-07-decision_tree/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://xooyong.github.io/posts/2025-07/2025-07-07-decision_tree/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://xooyong.github.io/posts/2025-07/2025-07-07-decision_tree/"><meta property="og:site_name" content="xooyong blog"><meta property="og:title" content="Decision Tree"><meta property="og:description" content="Decision Tree를 밑바닥부터 차근차근 알아보자..."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-07T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-07T00:00:00+00:00"><meta property="article:tag" content="ML"><meta property="og:image" content="https://xooyong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://xooyong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Decision Tree"><meta name=twitter:description content="Decision Tree를 밑바닥부터 차근차근 알아보자..."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://xooyong.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Decision Tree","item":"https://xooyong.github.io/posts/2025-07/2025-07-07-decision_tree/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Decision Tree","name":"Decision Tree","description":"Decision Tree를 밑바닥부터 차근차근 알아보자...","keywords":["ML"],"articleBody":"1. Deicison Tree Decision Tree는 질문이 있고 이 질문에 대해서 Yes, No로 대답하는 문제와 같다.\n이렇게 특정 기준(질문)에 따라 데이터를 나누는 것이 Decision Tree이다. 또한 이렇게 데이터를 분류하는 방식의 가장 큰 장점은 직관적 이라는 것이다. 직관적이라는 것은 사람이 해석 가능하다는 것이다.\n우리가 GPT, Claude와 같은 생성형AI, 복잡한 Deep Learning Model들은 사람이 해석하기가 힘들다는 것이다. 물론 완전히 해석하지 못하는 것은 아니지만, 가끔씩 왜 이런 결과가 나왔을지 추측하기 힘든 경우가 많다.\nML은 사람이 최대한 개입하지 않고 스스로 기계가 학습하게 하는 것이다. 그렇다면 이 알고리즘을 가지고 어떻게 자동으로 학습하게 할 수 있을까? 또한 supervised learning에서는 classification과 regression 문제가 있다 먼저 Classification Tree를 알아보자.\n2. Terminologies Root node: 처음 시작 노드 - child node만 가지고 있음 Parent node: 분할 전의 노드 Child node: 분할 후의 노드 Leaf nodes: 마지막 노드 - 결론 split criterion: 분할 할 때 사용한 변수와 값 - 질문, 분할 기준 데이터는 Root node에서부터 시작해서 Leaf node 까지 split criterion을 거치면서 분류될 것이다.\nExample Decision Tree에서 데이터가 분류되는 과정을 간단하게 살펴보자. 먼저 데이터는 “오늘 운동할까?“를 결정하는 것이다.\nindex X1: 날씨 X2: 온도 X3: 시간 운동여부(label) 0 맑음 높음 오전 ✅ 운동 1 맑음 높음 오후 ❌ 안함 2 맑음 낮음 오전 ✅ 운동 3 맑음 낮음 오후 ✅ 운동 4 비 높음 오전 ❌ 안함 5 비 높음 오후 ❌ 안함 6 비 낮음 오전 ❌ 안함 7 비 낮음 오후 ❌ 안함 이렇게 정형 데이터가 있을 때 날씨(X1), 온도(X2), 시간(X3)은 feature가 될 것이고, 운동여부는 label이다. 데이터 날씨(X1)을 기준으로 분류해보자. split criterion은 “날씨가 맑음?“으로 할 수 있을 것이다. 그렇다면 다음 node로 index 0, 1, 2, 3 날씨 맑음 데이터와 4, 5, 6, 7 비 데이터가 분류될 것이다. 그 다음은 온도(X2)를 기준으로 분류해보자 split criterion은 “온도가 높음?“으로 할 수 있다. 그렇다면 날씨가 맑은 데이터 0, 1, 2, 3에서 온도가 높은 데이터는 0, 1 데이터이므로 또 다음 node로 분류될 것이다. 이렇게 계속해서 leaf node에 도달할 것이다.\n날씨가 맑음?\r/ \\\rYES NO\r/ \\\r온도가 높음? 운동 안함 ❌\r/ \\\rYES NO\r/ \\\r시간이 오전? 운동함 ✅\r/ \\\rYES NO\r/ \\\r운동함 ✅ 운동 안함 ❌ 자 여기서 우리는 규칙을 추출할 수 있다.\n비가 오면 -\u003e 운동 안함 맑고 온도가 낮으면 -\u003e 운동함 맑고 온도가 높고 오전이면 -\u003e 운동함 맑고 온도가 높고 오후면 -\u003e 운동 안함 4번에서 맑고 온도가 높고 오후면 왜 운동을 안할까? 생각해보면 당연하게 온도가 높고 오후면 사람은 운동하기가 싫어질 것이다.\n이런 식으로 데이터에서 자동으로 규칙을 찾아내는 것이 Decision Tree의 핵심이다.\n3. Decision Tree - Geometric Perspective 우리는 지금까지 Decision Tree를 우리한테 익숙한 트리 형식으로 보았다. 하지만 우리는 기하학적으로 이해할 수 있다.\n이런식으로 데이터들을 split criterion(ex: 온도 \u003e 25)로 데이터들을 분류하고 이를 2차원애서 하나의 직선으로 생각할 수 있다.\n또한, 최종 결과를 보면 영역이 빨간색과 초록색으로 나누어 지는 걸 볼 수 있다. 영역이 이렇게 나누어지는 기준은 label이고 다수결로 결정된다.\n데이터들이 Decision Tree로 들어가고 split criterion을 기준으로 분류된다. 그리고 분류가 다 끝나면 데이터들은 leaf node에 있을 것이다. leaf node를 확인해보았을 때 label이 운동 안함(빨간색)이 많다면 그 leaf node의 label은 운동 안함이 될 것이다. 그러니까 온도 \u003c 25인 데이터들은 모두 운동 안함으로 학습한 것이고, 나중에 어떤 데이터가 들어왔을 때 온도가 25도 이하이면 운동 안함으로 분류할 수 있을것이다.\nclassification에서는 이렇게 label이 운동을 하냐, 안하냐, 개냐, 고양이냐 카테고리로 분류가 된다. 하지만 실제로 Decision Tree에서 leaf node에서의 class를 결정할 때 확률로 결정된다.\n어떤 leaf node에 데이터가 3개가 있고 각 데이터의 label이 x1(운동함), x2(운동 안함), x3(운동 안함) 이렇게 있다고 가정하자.\n그렇다면 여기서 운동함의 확률은 1/3, 운동 안함의 확률은 2/3로 표현할 수 있다.\n따라서 어떤 데이터 t1을 이렇게 학습된 Decision Tree에 넣고 이 leaf node에 도착했다면, 그 데이터 t1은 운동 안함이 될 것이고, 값은 2/3가 된다. 이 값은 나중에 loss를 계산할 때 사용될 것이다.\n또한, 확률로 계산하면 0.67, 0.33과 같이 더 세밀하게 표현할 수 있고, 단순히 우리가 분류할 때 확률이 0.5 이상이면 좋고, 0.5 이하이면 나쁘다라고 분류하면 문제가 발생할 수 있다. 확률이 0.51이여도 좋음이라고 분류되기 때문에 이는 다시 한번의 검증이 필요할 수도 있다.\n4. split criterion 데이터가 split criterion을 기준으로 나누어지고 leaf node에 도달했을 때 label에 따라 다수결로 결정되고 이는 결국 확률로 계산된다는 것을 알았다. 그러면 split criterion 이 분할 기준을 어떻게 정하고 찾는 것일까?\n원래 아주 기본적으로는 위에서 본 정형 데이터에서 어떤 featrue를 기준으로 그 값들 사이에 선을 하나씩 그어보는 것이다.\nX1 0.1 0.4 0.6 아주 간단하게 이런 데이터가 있을 때 0.1, 0.4 사이를 기준으로 나누어 보고, 0.4, 0.6 사이를 기준으로 나누어 보고… 이런 식으로 분류될 것이다.\n하지만 데이터가 매우 커진다면 이런 식으로 하나씩 선을 그어 보고 최적의 split criterion을 찾는다면 매우 많은 시간과 computing power가 필요할 것이다.\nOptimization 우리는 최적의 split criterion을 찾으려면 먼저 최적화(Optimization)을 알아보자.\n최적화(Optimization) 문제란 어떤 목적함수(Objective function)의 함수값을 최적화(최대화 또는 최소화)시키는 파라미터(변수) 조합을 찾는 문제이다.\n최적화(optimization) 문제는 크게 최대화(maximization) 문제와 최소화(minimization) 문제로 나눌 수 있고, 목적함수가 이윤, 점수 등인 경우에는 최대화 문제, 비용, 손실, 에러 등인 경우에는 최소화 문제가 된다.\n자 여기서 우리가 최적화할 기준 함수는 Loss Function으로 정의될 것이고, 이 기준 함수를 최소화 또는 최대화 하는 방향으로 ML 알고리즘을 학습시킨다.\n결론적으로, 기준 함수(Loss Function)을 최대화 또는 최소화 하는 Split criterion을 찾는 것이다.\n5. 그래서 최적의 split criterion을 찾는 방법은? Entropy Decision Tree에서 node가 분류가 잘 됬는지 안 됬는지 알아보기 위해서 보통 Entropy를 사용한다.\n$$ H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i) $$\nH(S): 집합 S의 엔트로피 c: 클래스 개수 p_i: 클래스 i의 비율 Entropy는 node의 불순도(impurity)를 측정하는 것이다.\nEntropy가 높다라는 의미는 해당 node에서 class의 비율이 불확실하다 라는 것이고 그러니까 class가 0과 1이 있을 때 50:50으로 있다는 것이다. 이를 불확실성이 높다라고도 한다.\nEntropy가 낮다라는 의미는 해당 node에서 class의 비율이 순수하다는 것이고 100:0 또는 0:100 이라면 Entropy=0 이라는 것이다.\nInformation Gain 위 사진에서 선이 split criterion이 되는 것이고 우리는 저 선을 보았을 때 분류가 잘 됬다고 보통 느낄 수 있다. 하지만 컴퓨터는 알 수 없다. 그래서 우리가 수학적으로 정의해서 기준을 주는 것이다.\n이렇게 선을 그었을 떄 정말 잘 나누어 졌을까를 알아보고 이를 모델이 학습시키게 하는 것이 우리의 목표이다.\n우리는 Information Gain(정보 이론)을 통해서 정말 데이터들이 잘 나누어 졌을까를 측정할 수 있다.\n$$ \\max_{j,s} \\text{IG}(j,s) = H(S) - \\left[ \\frac{|L_1|}{|S|} H(L_1) + \\frac{|L_2|}{|S|} H(L_2) \\right] $$\n$j$: Feature (온도, 습도) $s$: Split threshold (분할점) $S$: Root Node (전체 데이터) $L_1$: Left childe node (왼쪽 분할 영역) $j$: Right child node (오른쪽 분할 영역) $H(S)$: Root Node의 Entropy $H(L_1)$, $H(L_2)$: 각 분할 영역의 Entropy 목적 함수는 IG가 되는 것이고 이걸 최대화 하는 것이 목표다.\n위에서 볼 수 있듯이 온도를 기준으로 세로로 선을 그어서 분할했을 떄는 각 분할된 node에서의 Entropy가 높다. Entropy가 높다 -\u003e 해당 node에서의 불확실성이 높다 -\u003e class의 분포가 50:50에 가깝다.\n하지만 습도를 기준으로 가로로 선을 그어서 분할하면 각 node에서의 Entropy가 0이 된다. Entropy가 낮다 -\u003e 해당 node에서의 불확실성이 낮다. -\u003e class가 섞여있지 않고 하나의 class만 있다.\n이렇게 우리는 데이터를 최적의 split criterion을 찾아서 잘 분류할 수 있게 된다!\nGini Index Entropy 말고도 더 좋은 분할 기준이 있다.\n$$ \\text{Gini}(S) = 1 - \\sum_{i=1}^{c} p_i^2 $$\nCPU는 log 연산을 직접적으로 하지 못한다. 대신 테일러 급수나 룩업 테이블을 사용해서 계산을 한다.\nx를 2^k × m 형태로 분해 (k는 정수, 1 ≤ m \u003c 2) 룩업 테이블에서 log(m) 근사값 찾기 log₂(x) = k + log₂(m) 계산 여러 번의 덧셈/곱셈 반복 이런 식의 연산을 하기 때문에 Entropy의 log 함수에서 연산이 느려진다. 하지만 gini는 제곱 연산을 하기 때문에 log 연산보다 훨씬 빠르다. 제곱 연산은 단순 곱셈 2번이기 때문에 시간 복잡도 O(1)만에 처리 할 수 있다. 또한 값의 표현 범위도 Entropy는 0과 1사이 이지만 gini는 0과 0.5사이에서 표현된다.\n6. Regression Tree Classification에서는 데이터의 label 형태가 categorical 했다. 이는 데이터가 0이냐 1이냐, 개냐 고양이냐, 이렇게 딱 떨어지게 분류할 수 있다는 뜻이다.\nRegression 문제는 데이터가 이렇게 카테고리컬 하지 않다. 어떤 실수로 표현되있다. 이는 금액, 점수 등 이렇게 숫자로 표현된다는 것이다.\nclassification tree에서는 다수결 즉, leaf node에서 가장 많은 label에 따라 예측값 class가 결정됬다.\nregression tree에서는 class들의 형태가 숫자로 되어 있다고 했다. 다수결과 가장 비슷한 방법인 평균을 사용하는 것이다.\nleaf node 내에 데이터가 3개가 있고 각 데이터의 label이 0.1, 0.2, 0.3이라고 생각해보자.\n이것의 평균은 (0.1 + 0.2 + 0.3) / 3 = 0.167이 될것이다.\n이 뜻은 어떤 다른 데이터가 이 regression tree로 들어오고 위 leaf node에 들어왔을 때 예측값이 0.167이 된다는 것이다.\nMSE or Variance regression tree에서 split criterion을 구할 때 IG 사용한다. 하지만 분할 기준은 Entropy가 아닌 MSE를 사용한다.\n또한 이 MSE로 생각했을 때 우리는 분산(Variance)로도 접근할 수 있다.\n$$ \\text{MSE}(S) = \\frac{1}{|S|} \\sum_{i=1}^{|S|} (y_i - \\bar{y})^2 $$\n이를 IG에 적용시키면\n$$ \\text{MSE Reduction} = \\text{MSE}(S) - \\left[ \\frac{|L_1|}{|S|} \\text{MSE}(L_1) + \\frac{|L_2|}{|S|} \\text{MSE}(L_2) \\right] $$\n분할 전 MSE는\n분할 후\nclassification과 같이 목적 함수를 최대화 하는 방향으로 최적화 할 것이다.\nConclusion Decision Tree의 가장 큰 장점은 직관적이라는 것이고 사람이 해석 가능하다는 것이다.\n사람이 해석 가능하다면 우리는 왜 이런 결과가 나오고 이 결과가 나오기까지 과정을 사람이 이해할 수 있다는 것이다. 이는 문제와 원인을 분석해서 모델을 더 좋은 방향으로 분석하고 개선할 수 있다는 것이다.\n하지만 이런 Decision Tree에서도 문제가 있다. 단일 Decision Tree로는 깊이가 깊어되면 그 만큼 트리를 많이 분할 한다는 것이고 이렇게 되면 트리는 noise까지 학습하게 되어 Overfitting문제에 직면하게 된다.\n이를 해결하기 위해 Purning을 통해서 너무 분할이 되지 않게 막을 수 있다. 또한 이왕이면 복잡한 모델보다 단순한 모델을 구축하는 것이 좋을 것이다.\n또 다른 문제로는 불안정성(High Variance)가 있다. 이는 훈련 데이터(train data)가 조금만 바뀌어도 완전히 다른 트리를 생성할 수 있다.\n그리고 마지막으로 우리가 트리를 2차원으로 표현할 수 있어 사람이 해석하기 쉽지만, 대각선이나 곡선 경계를 표현하지 못한다는 것이다.\n이러한 문제들을 해결하기 위해 나온 Random Forest는 다음에 알아보도록 하자.\nRefernece DSBA ","wordCount":"1498","inLanguage":"en","image":"https://xooyong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-07-07T00:00:00Z","dateModified":"2025-07-07T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xooyong.github.io/posts/2025-07/2025-07-07-decision_tree/"},"publisher":{"@type":"Organization","name":"xooyong blog","logo":{"@type":"ImageObject","url":"https://xooyong.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xooyong.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://xooyong.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://xooyong.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://xooyong.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://xooyong.github.io/archives/ title=archives><span>archives</span></a></li><li><a href=https://xooyong.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://xooyong.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://xooyong.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Decision Tree</h1><div class=post-description>Decision Tree를 밑바닥부터 차근차근 알아보자...</div><div class=post-meta><span title='2025-07-07 00:00:00 +0000 UTC'>July 7, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1498 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/2025-07/2025-07-07-Decision_Tree.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=post-content><h2 id=1-deicison-tree>1. Deicison Tree<a hidden class=anchor aria-hidden=true href=#1-deicison-tree>#</a></h2><p>Decision Tree는 질문이 있고 이 질문에 대해서 Yes, No로 대답하는 문제와 같다.</p><p align=center><img src=/images/Decision_Tree/decision-tree.png width=50% alt="Decision Tree"></p><p>이렇게 특정 기준(질문)에 따라 데이터를 나누는 것이 Decision Tree이다. 또한 이렇게 데이터를 분류하는 방식의 가장 큰 장점은 <strong>직관적</strong> 이라는 것이다. 직관적이라는 것은 사람이 해석 가능하다는 것이다.</p><p>우리가 GPT, Claude와 같은 생성형AI, 복잡한 Deep Learning Model들은 사람이 해석하기가 힘들다는 것이다. 물론 완전히 해석하지 못하는 것은 아니지만, 가끔씩 왜 이런 결과가 나왔을지 추측하기 힘든 경우가 많다.</p><p>ML은 사람이 최대한 개입하지 않고 스스로 기계가 학습하게 하는 것이다. 그렇다면 이 알고리즘을 가지고 어떻게 자동으로 학습하게 할 수 있을까? 또한 supervised learning에서는 classification과 regression 문제가 있다 먼저 Classification Tree를 알아보자.</p><h2 id=2-terminologies>2. Terminologies<a hidden class=anchor aria-hidden=true href=#2-terminologies>#</a></h2><ul><li>Root node: 처음 시작 노드 - child node만 가지고 있음</li><li>Parent node: 분할 전의 노드</li><li>Child node: 분할 후의 노드</li><li>Leaf nodes: 마지막 노드 - 결론</li><li>split criterion: 분할 할 때 사용한 변수와 값 - 질문, 분할 기준</li></ul><p>데이터는 Root node에서부터 시작해서 Leaf node 까지 split criterion을 거치면서 분류될 것이다.</p><h3 id=example>Example<a hidden class=anchor aria-hidden=true href=#example>#</a></h3><p>Decision Tree에서 데이터가 분류되는 과정을 간단하게 살펴보자.
먼저 데이터는 &ldquo;오늘 운동할까?&ldquo;를 결정하는 것이다.</p><table><thead><tr><th>index</th><th>X1: 날씨</th><th>X2: 온도</th><th>X3: 시간</th><th>운동여부(label)</th></tr></thead><tbody><tr><td>0</td><td>맑음</td><td>높음</td><td>오전</td><td>✅ 운동</td></tr><tr><td>1</td><td>맑음</td><td>높음</td><td>오후</td><td>❌ 안함</td></tr><tr><td>2</td><td>맑음</td><td>낮음</td><td>오전</td><td>✅ 운동</td></tr><tr><td>3</td><td>맑음</td><td>낮음</td><td>오후</td><td>✅ 운동</td></tr><tr><td>4</td><td>비</td><td>높음</td><td>오전</td><td>❌ 안함</td></tr><tr><td>5</td><td>비</td><td>높음</td><td>오후</td><td>❌ 안함</td></tr><tr><td>6</td><td>비</td><td>낮음</td><td>오전</td><td>❌ 안함</td></tr><tr><td>7</td><td>비</td><td>낮음</td><td>오후</td><td>❌ 안함</td></tr></tbody></table><p>이렇게 정형 데이터가 있을 때 날씨(X1), 온도(X2), 시간(X3)은 feature가 될 것이고, 운동여부는 label이다. 데이터 날씨(X1)을 기준으로 분류해보자. split criterion은 &ldquo;날씨가 맑음?&ldquo;으로 할 수 있을 것이다. 그렇다면 다음 node로 index 0, 1, 2, 3 날씨 맑음 데이터와 4, 5, 6, 7 비 데이터가 분류될 것이다.
그 다음은 온도(X2)를 기준으로 분류해보자 split criterion은 &ldquo;온도가 높음?&ldquo;으로 할 수 있다. 그렇다면 날씨가 맑은 데이터 0, 1, 2, 3에서 온도가 높은 데이터는 0, 1 데이터이므로 또 다음 node로 분류될 것이다. 이렇게 계속해서 leaf node에 도달할 것이다.</p><pre tabindex=0><code>                    날씨가 맑음?
                   /            \
                 YES             NO
                /                  \
            온도가 높음?          운동 안함 ❌
           /          \
         YES           NO
        /               \
   시간이 오전?        운동함 ✅
   /        \
 YES         NO
 /            \
운동함 ✅    운동 안함 ❌
</code></pre><p>자 여기서 우리는 규칙을 추출할 수 있다.</p><ol><li>비가 오면 -> 운동 안함</li><li>맑고 온도가 낮으면 -> 운동함</li><li>맑고 온도가 높고 오전이면 -> 운동함</li><li>맑고 온도가 높고 오후면 -> 운동 안함</li></ol><p>4번에서 맑고 온도가 높고 오후면 왜 운동을 안할까? 생각해보면 당연하게 온도가 높고 오후면 사람은 운동하기가 싫어질 것이다.</p><p>이런 식으로 데이터에서 자동으로 규칙을 찾아내는 것이 Decision Tree의 핵심이다.</p><h2 id=3-decision-tree---geometric-perspective>3. Decision Tree - Geometric Perspective<a hidden class=anchor aria-hidden=true href=#3-decision-tree---geometric-perspective>#</a></h2><p>우리는 지금까지 Decision Tree를 우리한테 익숙한 트리 형식으로 보았다.
하지만 우리는 기하학적으로 이해할 수 있다.</p><iframe src=/code/Decision_Tree/decision_tree_2d.html class=fullwidth-viz height=750></iframe><p>이런식으로 데이터들을 split criterion(ex: 온도 > 25)로 데이터들을 분류하고 이를 2차원애서 하나의 직선으로 생각할 수 있다.</p><p>또한, 최종 결과를 보면 영역이 빨간색과 초록색으로 나누어 지는 걸 볼 수 있다. 영역이 이렇게 나누어지는 기준은 <strong>label</strong>이고 <strong>다수결</strong>로 결정된다.</p><p>데이터들이 Decision Tree로 들어가고 split criterion을 기준으로 분류된다. 그리고 분류가 다 끝나면 데이터들은 leaf node에 있을 것이다. leaf node를 확인해보았을 때 label이 운동 안함(빨간색)이 많다면 그 leaf node의 label은 운동 안함이 될 것이다. 그러니까 온도 &lt; 25인 데이터들은 모두 운동 안함으로 학습한 것이고, 나중에 어떤 데이터가 들어왔을 때 온도가 25도 이하이면 운동 안함으로 분류할 수 있을것이다.</p><p>classification에서는 이렇게 label이 운동을 하냐, 안하냐, 개냐, 고양이냐 카테고리로 분류가 된다. 하지만 실제로 Decision Tree에서 leaf node에서의 class를 결정할 때 <strong>확률</strong>로 결정된다.</p><p>어떤 leaf node에 데이터가 3개가 있고 각 데이터의 label이 x1(운동함), x2(운동 안함), x3(운동 안함) 이렇게 있다고 가정하자.</p><p>그렇다면 여기서 운동함의 확률은 1/3, 운동 안함의 확률은 2/3로 표현할 수 있다.</p><p>따라서 어떤 데이터 t1을 이렇게 학습된 Decision Tree에 넣고 이 leaf node에 도착했다면, 그 데이터 t1은 운동 안함이 될 것이고, 값은 2/3가 된다. 이 값은 나중에 loss를 계산할 때 사용될 것이다.</p><p>또한, 확률로 계산하면 0.67, 0.33과 같이 더 세밀하게 표현할 수 있고, 단순히 우리가 분류할 때 확률이 0.5 이상이면 좋고, 0.5 이하이면 나쁘다라고 분류하면 문제가 발생할 수 있다. 확률이 0.51이여도 좋음이라고 분류되기 때문에 이는 다시 한번의 검증이 필요할 수도 있다.</p><h2 id=4-split-criterion>4. split criterion<a hidden class=anchor aria-hidden=true href=#4-split-criterion>#</a></h2><p>데이터가 split criterion을 기준으로 나누어지고 leaf node에 도달했을 때 label에 따라 다수결로 결정되고 이는 결국 확률로 계산된다는 것을 알았다. 그러면 split criterion 이 분할 기준을 어떻게 정하고 찾는 것일까?</p><p>원래 아주 기본적으로는 위에서 본 정형 데이터에서 어떤 featrue를 기준으로 그 값들 사이에 선을 하나씩 그어보는 것이다.</p><table><thead><tr><th>X1</th></tr></thead><tbody><tr><td>0.1</td></tr><tr><td>0.4</td></tr><tr><td>0.6</td></tr></tbody></table><p>아주 간단하게 이런 데이터가 있을 때 0.1, 0.4 사이를 기준으로 나누어 보고, 0.4, 0.6 사이를 기준으로 나누어 보고&mldr; 이런 식으로 분류될 것이다.</p><p align=center><img src="/images/Decision_Tree/스크린샷 2025-09-07 151732.png" fullwidth-viz></p><p>하지만 데이터가 매우 커진다면 이런 식으로 하나씩 선을 그어 보고 최적의 split criterion을 찾는다면 매우 많은 시간과 computing power가 필요할 것이다.</p><h3 id=optimization>Optimization<a hidden class=anchor aria-hidden=true href=#optimization>#</a></h3><p>우리는 최적의 split criterion을 찾으려면 먼저 최적화(Optimization)을 알아보자.</p><p>최적화(Optimization) 문제란 어떤 목적함수(Objective function)의 함수값을 최적화(최대화 또는 최소화)시키는 파라미터(변수) 조합을 찾는 문제이다.</p><p>최적화(optimization) 문제는 크게 최대화(maximization) 문제와 최소화(minimization) 문제로 나눌 수 있고, 목적함수가 이윤, 점수 등인 경우에는 최대화 문제, 비용, 손실, 에러 등인 경우에는 최소화 문제가 된다.</p><p>자 여기서 우리가 최적화할 기준 함수는 Loss Function으로 정의될 것이고, 이 기준 함수를 최소화 또는 최대화 하는 방향으로 ML 알고리즘을 학습시킨다.</p><p>결론적으로, 기준 함수(Loss Function)을 최대화 또는 최소화 하는 Split criterion을 찾는 것이다.</p><h2 id=5-그래서-최적의-split-criterion을-찾는-방법은>5. 그래서 최적의 split criterion을 찾는 방법은?<a hidden class=anchor aria-hidden=true href=#5-그래서-최적의-split-criterion을-찾는-방법은>#</a></h2><h3 id=entropy>Entropy<a hidden class=anchor aria-hidden=true href=#entropy>#</a></h3><p>Decision Tree에서 node가 분류가 잘 됬는지 안 됬는지 알아보기 위해서 보통 Entropy를 사용한다.</p><p>$$
H(S) = -\sum_{i=1}^{c} p_i \log_2(p_i)
$$</p><ul><li>H(S): 집합 S의 엔트로피</li><li>c: 클래스 개수</li><li>p_i: 클래스 i의 비율</li></ul><p>Entropy는 node의 불순도(impurity)를 측정하는 것이다.</p><p>Entropy가 높다라는 의미는 해당 node에서 class의 비율이 불확실하다 라는 것이고 그러니까 class가 0과 1이 있을 때 50:50으로 있다는 것이다. 이를 불확실성이 높다라고도 한다.</p><p>Entropy가 낮다라는 의미는 해당 node에서 class의 비율이 순수하다는 것이고 100:0 또는 0:100 이라면 Entropy=0 이라는 것이다.</p><h3 id=information-gain>Information Gain<a hidden class=anchor aria-hidden=true href=#information-gain>#</a></h3><p align=center><img src="/images/Decision_Tree/스크린샷 2025-09-07 160920.png" width=50% alt="Decision Tree"></p><p>위 사진에서 선이 split criterion이 되는 것이고 우리는 저 선을 보았을 때 분류가 잘 됬다고 보통 느낄 수 있다. 하지만 컴퓨터는 알 수 없다. 그래서 우리가 수학적으로 정의해서 기준을 주는 것이다.</p><p>이렇게 선을 그었을 떄 정말 잘 나누어 졌을까를 알아보고 이를 모델이 학습시키게 하는 것이 우리의 목표이다.</p><p>우리는 Information Gain(정보 이론)을 통해서 정말 데이터들이 잘 나누어 졌을까를 측정할 수 있다.</p><p>$$
\max_{j,s} \text{IG}(j,s) = H(S) - \left[ \frac{|L_1|}{|S|} H(L_1) + \frac{|L_2|}{|S|} H(L_2) \right]
$$</p><ul><li>$j$: Feature (온도, 습도)</li><li>$s$: Split threshold (분할점)</li><li>$S$: Root Node (전체 데이터)</li><li>$L_1$: Left childe node (왼쪽 분할 영역)</li><li>$j$: Right child node (오른쪽 분할 영역)</li><li>$H(S)$: Root Node의 Entropy</li><li>$H(L_1)$, $H(L_2)$: 각 분할 영역의 Entropy</li></ul><p>목적 함수는 IG가 되는 것이고 이걸 최대화 하는 것이 목표다.</p><iframe src=/code/Decision_Tree/entropy.html class=fullwidth-viz height=750></iframe><p>위에서 볼 수 있듯이 온도를 기준으로 세로로 선을 그어서 분할했을 떄는 각 분할된 node에서의 Entropy가 높다. Entropy가 높다 -> 해당 node에서의 불확실성이 높다 -> class의 분포가 50:50에 가깝다.</p><p>하지만 습도를 기준으로 가로로 선을 그어서 분할하면 각 node에서의 Entropy가 0이 된다. Entropy가 낮다 -> 해당 node에서의 불확실성이 낮다. -> class가 섞여있지 않고 하나의 class만 있다.</p><p>이렇게 우리는 데이터를 최적의 split criterion을 찾아서 잘 분류할 수 있게 된다!</p><h3 id=gini-index>Gini Index<a hidden class=anchor aria-hidden=true href=#gini-index>#</a></h3><p>Entropy 말고도 더 좋은 분할 기준이 있다.</p><p>$$
\text{Gini}(S) = 1 - \sum_{i=1}^{c} p_i^2
$$</p><p>CPU는 log 연산을 직접적으로 하지 못한다. 대신 테일러 급수나 룩업 테이블을 사용해서 계산을 한다.</p><ol><li>x를 2^k × m 형태로 분해 (k는 정수, 1 ≤ m &lt; 2)</li><li>룩업 테이블에서 log(m) 근사값 찾기</li><li>log₂(x) = k + log₂(m) 계산</li><li>여러 번의 덧셈/곱셈 반복</li></ol><p>이런 식의 연산을 하기 때문에 Entropy의 log 함수에서 연산이 느려진다.
하지만 gini는 제곱 연산을 하기 때문에 log 연산보다 훨씬 빠르다. 제곱 연산은 단순 곱셈 2번이기 때문에 시간 복잡도 O(1)만에 처리 할 수 있다.
또한 값의 표현 범위도 Entropy는 0과 1사이 이지만 gini는 0과 0.5사이에서 표현된다.</p><h2 id=6-regression-tree>6. Regression Tree<a hidden class=anchor aria-hidden=true href=#6-regression-tree>#</a></h2><p>Classification에서는 데이터의 label 형태가 categorical 했다. 이는 데이터가 0이냐 1이냐, 개냐 고양이냐, 이렇게 딱 떨어지게 분류할 수 있다는 뜻이다.</p><p>Regression 문제는 데이터가 이렇게 카테고리컬 하지 않다. 어떤 실수로 표현되있다. 이는 금액, 점수 등 이렇게 숫자로 표현된다는 것이다.</p><p align=center><img src="/images/Decision_Tree/스크린샷 2025-09-07 183207.png" width=50% alt="Decision Tree"></p><p>classification tree에서는 다수결 즉, leaf node에서 가장 많은 label에 따라 예측값 class가 결정됬다.</p><p>regression tree에서는 class들의 형태가 숫자로 되어 있다고 했다. 다수결과 가장 비슷한 방법인 <strong>평균</strong>을 사용하는 것이다.</p><p>leaf node 내에 데이터가 3개가 있고 각 데이터의 label이 0.1, 0.2, 0.3이라고 생각해보자.</p><p>이것의 평균은 (0.1 + 0.2 + 0.3) / 3 = 0.167이 될것이다.</p><p>이 뜻은 어떤 다른 데이터가 이 regression tree로 들어오고 위 leaf node에 들어왔을 때 예측값이 0.167이 된다는 것이다.</p><h3 id=mse-or-variance>MSE or Variance<a hidden class=anchor aria-hidden=true href=#mse-or-variance>#</a></h3><p>regression tree에서 split criterion을 구할 때 IG 사용한다. 하지만 분할 기준은 Entropy가 아닌 MSE를 사용한다.</p><p>또한 이 MSE로 생각했을 때 우리는 분산(Variance)로도 접근할 수 있다.</p><p>$$
\text{MSE}(S) = \frac{1}{|S|} \sum_{i=1}^{|S|} (y_i - \bar{y})^2
$$</p><p>이를 IG에 적용시키면</p><p>$$
\text{MSE Reduction} = \text{MSE}(S) - \left[ \frac{|L_1|}{|S|} \text{MSE}(L_1) + \frac{|L_2|}{|S|} \text{MSE}(L_2) \right]
$$</p><p>분할 전 MSE는</p><p align=center><img src="/images/Decision_Tree/스크린샷 2025-09-07 191206.png" width=50% alt="Decision Tree"></p><p>분할 후</p><p align=center><img src="/images/Decision_Tree/스크린샷 2025-09-07 191244.png" width=50% alt="Decision Tree"></p><p>classification과 같이 목적 함수를 최대화 하는 방향으로 최적화 할 것이다.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Decision Tree의 가장 큰 장점은 <strong>직관적</strong>이라는 것이고 사람이 <strong>해석 가능</strong>하다는 것이다.</p><p>사람이 해석 가능하다면 우리는 왜 이런 결과가 나오고 이 결과가 나오기까지 과정을 사람이 이해할 수 있다는 것이다. 이는 문제와 원인을 분석해서 모델을 더 좋은 방향으로 분석하고 개선할 수 있다는 것이다.</p><p>하지만 이런 Decision Tree에서도 문제가 있다. 단일 Decision Tree로는 깊이가 깊어되면 그 만큼 트리를 많이 분할 한다는 것이고 이렇게 되면 트리는 noise까지 학습하게 되어 <strong>Overfitting</strong>문제에 직면하게 된다.</p><p>이를 해결하기 위해 <strong>Purning</strong>을 통해서 너무 분할이 되지 않게 막을 수 있다. 또한 이왕이면 복잡한 모델보다 단순한 모델을 구축하는 것이 좋을 것이다.</p><p>또 다른 문제로는 불안정성(High Variance)가 있다. 이는 훈련 데이터(train data)가 조금만 바뀌어도 완전히 다른 트리를 생성할 수 있다.</p><p>그리고 마지막으로 우리가 트리를 2차원으로 표현할 수 있어 사람이 해석하기 쉽지만, <strong>대각선</strong>이나 <strong>곡선 경계</strong>를 표현하지 못한다는 것이다.</p><p>이러한 문제들을 해결하기 위해 나온 Random Forest는 다음에 알아보도록 하자.</p><h3 id=refernece>Refernece<a hidden class=anchor aria-hidden=true href=#refernece>#</a></h3><ol><li><a href=https://github.com/pilsung-kang/Business-Analytics-ITS504-/tree/master>DSBA</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://xooyong.github.io/tags/ml/>ML</a></li></ul><nav class=paginav><a class=next href=https://xooyong.github.io/posts/2025-06/2025-06-24-about_machine_learning/><span class=title>Next »</span><br><span>About Machine Learning</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Decision Tree on x" href="https://x.com/intent/tweet/?text=Decision%20Tree&amp;url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-07-decision_tree%2f&amp;hashtags=ML"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Decision Tree on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-07-decision_tree%2f&amp;title=Decision%20Tree&amp;summary=Decision%20Tree&amp;source=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-07-decision_tree%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Decision Tree on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-07-decision_tree%2f&title=Decision%20Tree"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Decision Tree on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-07-decision_tree%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Decision Tree on whatsapp" href="https://api.whatsapp.com/send?text=Decision%20Tree%20-%20https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-07-decision_tree%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Decision Tree on telegram" href="https://telegram.me/share/url?text=Decision%20Tree&amp;url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-07-decision_tree%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Decision Tree on ycombinator" href="https://news.ycombinator.com/submitlink?t=Decision%20Tree&u=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-07-decision_tree%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://xooyong.github.io/>xooyong blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>