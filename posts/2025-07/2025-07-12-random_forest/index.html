<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Random Forest | xooyong blog</title><meta name=keywords content="ML"><meta name=description content="Ensemble Learning"><meta name=author content="Me"><link rel=canonical href=https://xooyong.github.io/posts/2025-07/2025-07-12-random_forest/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://xooyong.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://xooyong.github.io/posts/2025-07/2025-07-12-random_forest/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://xooyong.github.io/posts/2025-07/2025-07-12-random_forest/"><meta property="og:site_name" content="xooyong blog"><meta property="og:title" content="Random Forest"><meta property="og:description" content="Ensemble Learning"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-12T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-12T00:00:00+00:00"><meta property="article:tag" content="ML"><meta property="og:image" content="https://xooyong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://xooyong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Random Forest"><meta name=twitter:description content="Ensemble Learning"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://xooyong.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Random Forest","item":"https://xooyong.github.io/posts/2025-07/2025-07-12-random_forest/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Random Forest","name":"Random Forest","description":"Ensemble Learning","keywords":["ML"],"articleBody":"1. 단일 Deicision Tree의 한계 저번 글에서 Decision Tree에 대해서 자세히 알아보았다. Decision Tree는 직관적이고 사람이 해석 가능하며 매우 좋은 알고리즘인 건 분명하다. 하지만 몇 개의 문제가 있다.\n단일 Decision Tree의 가장 큰 문제 - Overfitting 데이터가 매우 복잡하고 해석하기 어려울 때 Deicision Tree에서 분할은 어떻게 될까? 데이터의 feature가 1개, 2개, 10개 이하 이렇게 feature의 수가 적다면 데이터를 분할하기 쉽고 빠르게 데이터를 분류할 수 있을 것이다. 하지만 feature가 100개, 1000개 점점 많아진다면 분할하는 횟수가 엄청나게 많아질 것이다. 트리는 모든 terminal node의 purity가 100%일 때까지 분할 할 것이다. 각 node에 한 종류의 class만 존재할 때 종료를 하게 되고 이를 full tree라고 부르며 이렇게 되면 overfitting의 문제가 생기고 generalization 성능이 저하되는 현상이 발생한다.\n우리가 원하는 것은 학습시킨 모델이 다른 데이터에 대해서도 성능이 높게 나오길 원한다. 즉, generalization 성능이 높아야 된다.\ngeneralization 성능이 저하된다는 것은 다른 데이터에 대해서는 성능이 낮다는 것이고 데이터가 조금이라도 달라지면 모델의 성능이 매우 민감하게 반응할 것이다.\nNo Free Lunch! 위에서 말한 overfitting 문제와 관련해서 아주 재밌는 이론이 있다. 이 이론의 핵심은 모든 가능한 문제에 대해 항상 최고 성능을 보이는 단일한 알고리즘은 존재하지 않는다는 것이다.\n어떤 알고리즘 A가 특정 문제 유형에서 알고리즘 B보다 더 좋은 성능을 낸다면, 반드시 알고리즘 B가 더 좋은 성능을 내는 다른 문제 유형이 존재한다는 것이다.\n하지만 위에서 말했듯이 우리가 원하는 건 다른 데이터에 대해서도 성능이 좋길 원한다. 우리는 모델이 우리가 해결하고자 하는 문제에 대해서도 성능이 좋길 바라고 모델이 보지 못한 데이터에 대해서도 성능이 좋길 바란다.\n이 이론에 따르면 이런 모든 데이터에 대해서 성능이 좋은 알고리즘은 없다는 것이다. 그렇다면 진짜 말하고자 하는 바가 무엇이고 우리에게 어떤 교훈을 주려는 것일까?\n다양한 모델을 시도해야 한다. 특정 문제에 어떤 알고리즘이 가장 적합한지 미리 알 수 없으므로, 여러 모델을 실험하고 성능을 비교 평가하는 과정이 필수적이다. 문제에 대한 이해가 우선이다. 성공적인 ML을 위해서는 데이터와 해결하려는 문제를 깊이 이해하고 그 특성에 맞는 알고리즘을 선택하는 것이 중요하다. 하이퍼파라미터 튜닝의 중요성 같은 알고리즘이라도 하이퍼파라미터를 어떻게 설정하느냐에 따라 성능이 크게 달라질 수 있다. 데이터에 맞게 모델을 세밀하게 조정하는 과정이 필요하다. 어쩃든 결국 공짜 점심 이론이 말하는 건 어떤 문제든 완벽하게 해결하는 단 하나의 만능 모델은 없다라는 것이다.\n이것이 바로 Ensemble 기법의 출발점이다. Ensemble은 “공짜 점심은 없다\"는 현실을 인정하고, 여러 모델을 협력시켜 최적의 점심을 만들어내자는 것이다.\n2. Ensemble Ensemble(앙상블)는 프랑스어로 함께, 동시에 라는 뜻을 가지고 있다. 또한 영어로는 합창단, 무용단 등을 의미한다.\n이 단어의 뜻에서도 알 수 있 듯이 Machine Learning에서 여러 개의 모델을 협력시키는 것이다.\n하나의 모델은 데이터의 변화에 민감할 수 있지만 다양한 관점에서 학습한 여러 개의 모델의 결과를 종합할 경우 좀 더 강건하고 정확한 결과를 얻을 수 있다.\nBias-Variance Trade-off 여기서 더 나아가면 Ensemble의 목적은 Variance의 감소에 의한 오류를 감소시키는 것이다.\n높은 Bias는 모델이 너무 단순해서 데이터의 패턴을 제대로 학습하지 못하는 상태(Underfitting)이다.\n화살들이 과녁의 정중앙에서 멀리 떨어진 곳에 일관되게 모여있는 경우이다. 조준 자체가 잘못된 것이다.\n높은 Variance는 모델이 훈련 데이터에 너무 과도하게 맞춰져, 새로운 데이터에 대한 예측이 매우 불안정한 상태(Overfitting)이다.\n화살들이 과녁의 정중앙 주변에 있긴 하지만, 매우 넓게 흩어져 있는 경우이다. 쏠 때마다 결과가 크게 흔들리는 것이다.\n모델의 복잡도를 높이면(Decision Tree의 깊이를 깊게 하면) 훈련 데이터를 더 세밀하게 학습하여 Bias는 낮아지지만, 데이터의 노이즈까지 학습하여 Varinace는 높아진다.\n반대로 모델을 너무 단순하게 만들면 Variance는 낮아지지만, 데이터의 패턴을 놓쳐 Bias는 높아진다.\n이 둘은 시소와 같아서, 하나를 낮추면 다른 하나가 올라가는 경향이 있다. 이것이 바로 Bias-Variance Trade-off이다. 머신러닝의 목표는 이 둘의 균형을 잘 맞춰 Total Error를 최소화하는 것이다.\n자세한 수학적인 증명은 다음에…\n그렇다면 Random Forest는 어떻게 이 문제를 해결하는가?\nRandom Forest의 기본 구성 요소는 Deicision Tree이다.\n깊이가 깊은 Decision Tree는 훈련 데이터를 거의 완벽하게 학습할 수 있다, 즉 Bias가 매우 낮다. 하지만 데이터의 작은 변화에도 구조가 크게 바뀌고 예측이 심하게 흔들리는, Variance가 매우 높은 모델이다.\nRandom Forest는 바로 이 낮은 Bias라는 장점을 유지하면서 높은 Variance라는 치명적인 단점을 해결하려고 한다.\n3. Random Forest의 두 가지 전략 Enmsemble의 Diversity(다양성) 확보를 위해 두 가지 전략이 있다.\nBootstrapp Aggregating (Bagging) Ensemble의 각 멤버(모델)은 서로 다른 학습 데이터셋을 이용한다.\n각 데이터셋은 sampling with replacement(복원 추출) 을 통해 원래 데이터의 수만큼의 크기를 갖도록 샘플링한다.\n개별 데이터셋을 Bootstrap이라고 부른다.\n이론적으로 하나의 관측치가 하나의 Bootstrap에 한 번도 선택되지 않을 확률은 $$ p = \\left( 1 - \\frac{1}{N} \\right)^{N} \\rightarrow \\lim_{N \\to \\infty} \\left( 1 - \\frac{1}{N} \\right)^{N} = e^{-1} = 0.368 $$\nBagging은 평균을 통해 Bias를 줄인다. 여러 개의 값을 측정해서 평균을 내면 원래 값에 더 가까워진다는 통계적 원리를 이용한다.\n서로 다른 훈련 데이터 샘플을 이용해 수백 개의 독립적인 Decision Tree를 만든다. (각 트리는 여전히 Variance가 높다) 예측 시, 이 모든 트리의 예측 결과를 모아 평균을 내거나 다수결 투표를 한다. A 트리의 예측 오류와 B 트리의 예측 오류는 서로 다른 방향을 튈 가능성이 있따. 따라서 Random Forest는 수백 개의 트리를 만들고 예측을 평균 내면, 이러한 개별적인 오류들이 서로 상쇄되면서 숲 전체의 Variance가 줄어들게 된다.\nRandom Subspace t 번쨰 트리를 살펴보자\n원래 변수들 중에서 모델 구축에 쓰일 입력 변수를 무작위로 선택한다. 선택된 입력 변수 중에 분할될 변수를 선택한다. 이러한 과정을 full-grown tree가 될 때까지 반복한다. Decision Tree의 분기점을 탐색할 때, 원래 변수의 수보다 적은 수의 변수를 임의로 선택하여 해당 변수들만 고려 대상으로 한다.\n만약 모든 트리가 같은 데이터와 같은 특성을 보고 학습한다면, 결국 비슷한 모양의 트리들만 만들어질 것이다. 이런 비슷한 트리들의 예측을 평균 낸다면 Variance 감소 효과는 미미할 것이다.\n다시 정리해보자.\n각 트리의 노드를 분할할 때마다, 전체 feature 중 일부만 무작위로 선택한다. 선택된 feature들 중에서만 최적의 split criterion을 찾도록 강제한다. 이 과정을 통해 각 트리들은 서로 다른 feature를 기반으로 성장하게 되어 모양이 더욱 다양해진다.(상관관계 감소) 이렇게 서로 다른 개성을 가진 트리들의 예측을 평균 낼 때 Variance 감소 효과는 커진다.\n4. Generalization Error 각각의 개별 트리는 Overfitting될 수 있다.\nRandom Forest는 트리수가 충분히 많을 때 Strong Law of Large Numbers에 의해 Overfitting되지 않고 그 error는 limiting value에 수렴된다.\n$$ \\text{Generalization error} \\leq \\frac{\\bar{\\rho}(1 - s^2)}{s^2} $$\n$\\bar{\\rho}$: Decision Tree 사이의 평균 상관관계 $s$: 올바로 예측한 트리와 잘못 예측한 트리 수 차이의 평균 개별 트리의 정확도가 높을수록 $s$는 증가한다.\nBagging과 Random Subspace는 각 모델들의 독립성, 일반화, 무작위성을 최대화 시켜 모델간의 상관관계 $\\rho$를 감소시킨다.\n개별 트리의 정확도, 독립성이 높을수록 Random Forest의 성능이 높아진다.\n5. Out of bag (OOB) Decision Tree는 직관적이고 사람이 해석 가능하다. Random Forest는 Decision Tree를 수백 개를 만들어낸다. 그리고 수백 개의 트리들을 평균을 낸다.\n이렇게 수백 개의 트리를 만들어내면 어떻게 이 알고리즘의 성능이 좋은지 안 좋은지를 평가할 수 있을까?\n또한, 직관적이고 해석 가능한 Deicision Tree의 장점이 사라지게 되는게 아닐까?\nOOB의 가장 중요한 용도는 모델 성능을 검증하는 것이다.\nOOB 검증의 전체 과정 1단계. 모델 훈련 Random Forest는 수백 개의 Decision Tree로 이루어진 숲이다. 각 트리를 훈련시킬 때, 전체 훈련 데이터가 아닌 Bootstrap Sample을 사용한다.\nsampling: 전체 훈련 데이터(100개)에서 중복을 허용하여 무작위로 데이터 100개를 뽑는다. OOB 샘플 발생: 이 과정에서 어떤 데이터는 여러 번 뽑히고, 어떤 데이터는 한 번도 뽑히지 않는다. 여기서 뽑히지 않는 데이터가 나올 확률은 위에서 설명했듯이 약 0.37이다. 예시 트리 1의 훈련 데이터(Bootstrap sample)에는 데이터 #5가 포함되지 않았다. 데이터 #5는 트리 1의 OOB 샘플이다. 트리 2의 훈련 데이터에는 데이터 #5가 포함되었다. 데이터 #5는 트리 2의 훈련 샘플(In-Bag)이다. 이 과정을 숲의 모든 트리에 대해 반복한다. 결과적으로, 모든 데이터는 어떤 트리에게는 훈련 데이터로 쓰이고, 다른 트리에게는 OOB 샘플로 남게 된다.\n2단계. OOB Prediction 이제 훈련이 끝났으니, 모델의 성능을 검증할 차례이다.\n여기서 단 한 가지 중요한 규칙이 있다.\n규칙: 각 데이터는 자신을 학습하는 데 사용하지 않았던 트리들에게만 가서 예측을 받는다. 예시 숲에 있는 500개의 트리가 있고 이 트리들 중에서 훈련할 때 데이터 #5를 사용하지 않았던 트리들(OOB 샘플로 가졌던 트리들)을 모두 찾아낸다.(트리1, 3, 6.. 499번 등 약 200개의 트리들을 찾았다고 해보자.) 이 트리들에게만 데이터 #5를 보여주고 예측을 진행한다. 이 트리들의 예측 결과를 종합한다(classification이면 다수결, regression이면 평균) 이렇게 얻은 최종 결론이 바로 데이터 #5에 대한 OOB 예측값이다. 이 과정을 훈련 데이터에서 모든 개별 데이터에 대해 반복한다.(데이터 #1부터 #100까지 각각의 OOB 예측값을 모두 구한다.)\n3단계. OOB Score 계산 이제 모든 데이터에 대한 OOB 예측값과 실제 정답을 확보했다. 마지막으로 이 둘을 비교하여 최종 성능을 계산한다.\n데이터 #1의 OOB 예측값 \u003c-\u003e 데이터 #1의 실제 정답 데이터 #2의 OOB 예측값 \u003c-\u003e 데이터 #2의 실제 정답 … 데이터 #100의 OOB 예측값 \u003c-\u003e 데이터 #100의 실제 정답 이 비교 결과를 바탕으로 모델의 전체적인 성능 지표(정확도, 정밀도, MSE 등)을 계산한다. 이렇게 계산된 최종 성능 지표가 바로 OOB Score이다.\nOOB Score는 모델이 처음 보는 데이터에 대해서 얼마나 잘 작동할지를 나타내는 신뢰할 수 있는 generalization 성능을 추정할 수 있다.\n6. Featrue Importance OOB를 통해 변수의 중요도를 구할 수 있다.\nRandom Forest에서 변수의 중요도가 높다면\nRandom permutation 전-후의 OOB Error 차이가 크게 나타나야 한다. Random permutation 전은 OOB data에서 어떤 변수의 관측치들을 섞기 전이고 이에 대한 OOB error $p_i$를 계산한다. Random permutation 후는 OOB data에서 어떤 변수의 관측치들을 무작위로 섞고 OOB error $e_i$를 계산한다. 차이를 구한다 $d_i^m = p_i^m - e_i^m$, (기준 성능) - (뒤섞은 후 성능) 이 차이가 클수록 그 변수가 모델의 예측에 중요하게 사용되었다는 의미다. 그 차이의 편차가 적어야 한다. m번째 트리에서 변수 i에 대한 Random permutation 전후 OOB error의 차이는\n$$d_i^m = p_i^m - e_i^m$$\n전체 트리들에 대한 OOB error 차이의 평균 및 분산\n$$\\overline{d}i = \\frac{1}{m} \\sum{i=1}^{m} d_i^m$$\n$$s_i^2 = \\frac{1}{m-1} \\sum_{i=1}^{m} (d_i^m - \\bar{d}_i)^2$$\ni번째 변수의 중요도\n$$v_i = \\frac{\\bar{d}_i}{s_i}$$\n이렇게 변수의 중요도를 측정하고 barplot으로 나타낸다면\nConclusion Random Forest는 Ensemble 기법인 Bagging 사용하여 단일 Decision Tree의 높은 Variance를 줄인다.\nVariance가 줄으면서 모델은 새로운 데이터에 대해서 민감하게 반응하지 않고 강건하고 generalization 성능이 높은 모델이 된다.\n또한, 단일 Decision Tree는 2차원으로 생각했을 때 이진 경계면을 생성하지만 Random Forest에서 수백 개의 Decision Tree를 만들고 모든 트리에 대해서 평균을 내므로 예측값이 연속형의 스코어 가깝게 생성된다.\n필자도 AI 대회에서 다양한 알고리즘에 대한 테스트를 수행하기 전에 Random forest를 baseline으로 먼저 적용해본다.\n학습 hyperparameter에 대해서 variance가 크게 낮은 모델이기 때문에 왠만한 문제에 있어서 좋은 성능을 보이기 때문이다.\n","wordCount":"1523","inLanguage":"en","image":"https://xooyong.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-07-12T00:00:00Z","dateModified":"2025-07-12T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xooyong.github.io/posts/2025-07/2025-07-12-random_forest/"},"publisher":{"@type":"Organization","name":"xooyong blog","logo":{"@type":"ImageObject","url":"https://xooyong.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xooyong.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://xooyong.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://xooyong.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://xooyong.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://xooyong.github.io/archives/ title=archives><span>archives</span></a></li><li><a href=https://xooyong.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://xooyong.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://xooyong.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Random Forest</h1><div class=post-description>Ensemble Learning</div><div class=post-meta><span title='2025-07-12 00:00:00 +0000 UTC'>July 12, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1523 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/2025-07/2025-07-12-Random_Forest.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=post-content><h2 id=1-단일-deicision-tree의-한계>1. 단일 Deicision Tree의 한계<a hidden class=anchor aria-hidden=true href=#1-단일-deicision-tree의-한계>#</a></h2><p>저번 글에서 Decision Tree에 대해서 자세히 알아보았다. Decision Tree는 직관적이고 사람이 해석 가능하며 매우 좋은 알고리즘인 건 분명하다.
하지만 몇 개의 문제가 있다.</p><h3 id=단일-decision-tree의-가장-큰-문제---overfitting>단일 Decision Tree의 가장 큰 문제 - Overfitting<a hidden class=anchor aria-hidden=true href=#단일-decision-tree의-가장-큰-문제---overfitting>#</a></h3><p>데이터가 매우 복잡하고 해석하기 어려울 때 Deicision Tree에서 분할은 어떻게 될까? 데이터의 feature가 1개, 2개, 10개 이하 이렇게 feature의 수가 적다면 데이터를 분할하기 쉽고 빠르게 데이터를 분류할 수 있을 것이다. 하지만 feature가 100개, 1000개 점점 많아진다면 분할하는 횟수가 엄청나게 많아질 것이다. 트리는 모든 terminal node의 purity가 100%일 때까지 분할 할 것이다. 각 node에 한 종류의 class만 존재할 때 종료를 하게 되고 이를 <strong>full tree</strong>라고 부르며 이렇게 되면 <strong>overfitting</strong>의 문제가 생기고 generalization 성능이 저하되는 현상이 발생한다.</p><p align=center><img src=/images/Random_Forest/dt_depth.png width=50% alt="Decision Tree"></p><p>우리가 원하는 것은 학습시킨 모델이 다른 데이터에 대해서도 성능이 높게 나오길 원한다. 즉, generalization 성능이 높아야 된다.</p><p>generalization 성능이 저하된다는 것은 다른 데이터에 대해서는 성능이 낮다는 것이고 데이터가 조금이라도 달라지면 모델의 성능이 매우 민감하게 반응할 것이다.</p><h3 id=no-free-lunch>No Free Lunch!<a hidden class=anchor aria-hidden=true href=#no-free-lunch>#</a></h3><p>위에서 말한 overfitting 문제와 관련해서 아주 재밌는 이론이 있다. 이 이론의 핵심은 <strong>모든 가능한 문제에 대해 항상 최고 성능을 보이는 단일한 알고리즘은 존재하지 않는다는 것</strong>이다.</p><p>어떤 알고리즘 A가 특정 문제 유형에서 알고리즘 B보다 더 좋은 성능을 낸다면, 반드시 알고리즘 B가 더 좋은 성능을 내는 다른 문제 유형이 존재한다는 것이다.</p><p>하지만 위에서 말했듯이 우리가 원하는 건 다른 데이터에 대해서도 성능이 좋길 원한다. 우리는 모델이 우리가 해결하고자 하는 문제에 대해서도 성능이 좋길 바라고 모델이 보지 못한 데이터에 대해서도 성능이 좋길 바란다.</p><p>이 이론에 따르면 이런 모든 데이터에 대해서 성능이 좋은 알고리즘은 없다는 것이다. 그렇다면 진짜 말하고자 하는 바가 무엇이고 우리에게 어떤 교훈을 주려는 것일까?</p><ol><li>다양한 모델을 시도해야 한다.<ul><li>특정 문제에 어떤 알고리즘이 가장 적합한지 미리 알 수 없으므로, 여러 모델을 실험하고 성능을 비교 평가하는 과정이 필수적이다.</li></ul></li><li>문제에 대한 이해가 우선이다.<ul><li>성공적인 ML을 위해서는 데이터와 해결하려는 문제를 깊이 이해하고 그 특성에 맞는 알고리즘을 선택하는 것이 중요하다.</li></ul></li><li>하이퍼파라미터 튜닝의 중요성<ul><li>같은 알고리즘이라도 하이퍼파라미터를 어떻게 설정하느냐에 따라 성능이 크게 달라질 수 있다. 데이터에 맞게 모델을 세밀하게 조정하는 과정이 필요하다.</li></ul></li></ol><p>어쩃든 결국 공짜 점심 이론이 말하는 건 <strong>어떤 문제든 완벽하게 해결하는 단 하나의 만능 모델은 없다</strong>라는 것이다.</p><p>이것이 바로 Ensemble 기법의 <strong>출발점</strong>이다. Ensemble은 &ldquo;공짜 점심은 없다"는 현실을 인정하고, 여러 모델을 <strong>협력</strong>시켜 최적의 점심을 만들어내자는 것이다.</p><h2 id=2-ensemble>2. Ensemble<a hidden class=anchor aria-hidden=true href=#2-ensemble>#</a></h2><p>Ensemble(앙상블)는 프랑스어로 함께, 동시에 라는 뜻을 가지고 있다. 또한 영어로는 합창단, 무용단 등을 의미한다.</p><p>이 단어의 뜻에서도 알 수 있 듯이 Machine Learning에서 여러 개의 모델을 협력시키는 것이다.</p><p>하나의 모델은 데이터의 변화에 민감할 수 있지만 다양한 관점에서 학습한 여러 개의 모델의 결과를 종합할 경우 좀 더 강건하고 정확한 결과를 얻을 수 있다.</p><h3 id=bias-variance-trade-off>Bias-Variance Trade-off<a hidden class=anchor aria-hidden=true href=#bias-variance-trade-off>#</a></h3><p>여기서 더 나아가면 Ensemble의 목적은 <strong>Variance의 감소</strong>에 의한 오류를 감소시키는 것이다.</p><p><strong>높은 Bias</strong>는 모델이 너무 단순해서 데이터의 패턴을 제대로 학습하지 못하는 상태(Underfitting)이다.</p><p>화살들이 과녁의 정중앙에서 멀리 떨어진 곳에 <strong>일관되게</strong> 모여있는 경우이다. 조준 자체가 잘못된 것이다.</p><p><strong>높은 Variance</strong>는 모델이 훈련 데이터에 너무 과도하게 맞춰져, 새로운 데이터에 대한 예측이 매우 불안정한 상태(Overfitting)이다.</p><p>화살들이 과녁의 정중앙 주변에 있긴 하지만, <strong>매우 넓게 흩어져</strong> 있는 경우이다. 쏠 때마다 결과가 크게 흔들리는 것이다.</p><p>모델의 복잡도를 높이면(Decision Tree의 깊이를 깊게 하면) 훈련 데이터를 더 세밀하게 학습하여 Bias는 낮아지지만, 데이터의 노이즈까지 학습하여 Varinace는 높아진다.</p><p>반대로 모델을 너무 단순하게 만들면 Variance는 낮아지지만, 데이터의 패턴을 놓쳐 Bias는 높아진다.</p><p>이 둘은 시소와 같아서, 하나를 낮추면 다른 하나가 올라가는 경향이 있다. 이것이 바로 Bias-Variance Trade-off이다. 머신러닝의 목표는 이 둘의 균형을 잘 맞춰 Total Error를 최소화하는 것이다.</p><p><span style=color:gray>자세한 수학적인 증명은 다음에&mldr;</span></p><p>그렇다면 Random Forest는 어떻게 이 문제를 해결하는가?</p><p>Random Forest의 기본 구성 요소는 Deicision Tree이다.</p><p>깊이가 깊은 Decision Tree는 훈련 데이터를 거의 완벽하게 학습할 수 있다, 즉 <strong>Bias가 매우 낮다</strong>. 하지만 데이터의 작은 변화에도 구조가 크게 바뀌고 예측이 심하게 흔들리는, <strong>Variance가 매우 높은</strong> 모델이다.</p><p>Random Forest는 바로 이 낮은 Bias라는 장점을 유지하면서 높은 Variance라는 치명적인 단점을 해결하려고 한다.</p><h2 id=3-random-forest의-두-가지-전략>3. Random Forest의 두 가지 전략<a hidden class=anchor aria-hidden=true href=#3-random-forest의-두-가지-전략>#</a></h2><p>Enmsemble의 Diversity(다양성) 확보를 위해 두 가지 전략이 있다.</p><h3 id=bootstrapp-aggregating-bagging>Bootstrapp Aggregating (Bagging)<a hidden class=anchor aria-hidden=true href=#bootstrapp-aggregating-bagging>#</a></h3><p>Ensemble의 각 멤버(모델)은 서로 다른 학습 데이터셋을 이용한다.</p><p>각 데이터셋은 <strong>sampling with replacement(복원 추출)</strong> 을 통해 원래 데이터의 수만큼의 크기를 갖도록 샘플링한다.</p><p>개별 데이터셋을 <strong>Bootstrap</strong>이라고 부른다.</p><p align=center><img src=/images/Random_Forest/bagging1.png width=50% alt="Decision Tree"></p><p>이론적으로 하나의 관측치가 하나의 Bootstrap에 한 번도 선택되지 않을 확률은
$$
p = \left( 1 - \frac{1}{N} \right)^{N} \rightarrow \lim_{N \to \infty} \left( 1 - \frac{1}{N} \right)^{N} = e^{-1} = 0.368
$$</p><p>Bagging은 평균을 통해 Bias를 줄인다. 여러 개의 값을 측정해서 평균을 내면 원래 값에 더 가까워진다는 통계적 원리를 이용한다.</p><ol><li>서로 다른 훈련 데이터 샘플을 이용해 수백 개의 <strong>독립적인 Decision Tree</strong>를 만든다. (각 트리는 여전히 Variance가 높다)</li><li>예측 시, 이 모든 트리의 예측 결과를 모아 평균을 내거나 다수결 투표를 한다.</li></ol><p>A 트리의 예측 오류와 B 트리의 예측 오류는 서로 다른 방향을 튈 가능성이 있따. 따라서 Random Forest는 수백 개의 트리를 만들고 예측을 평균 내면, 이러한 개별적인 오류들이 <strong>서로 상쇄되면서</strong> 숲 전체의 Variance가 줄어들게 된다.</p><h3 id=random-subspace>Random Subspace<a hidden class=anchor aria-hidden=true href=#random-subspace>#</a></h3><p>t 번쨰 트리를 살펴보자</p><p align=center><img src=/images/Random_Forest/random_subspace1.png width=50% alt="Decision Tree"></p><ol><li>원래 변수들 중에서 모델 구축에 쓰일 입력 변수를 <strong>무작위</strong>로 선택한다.</li></ol><p align=center><img src=/images/Random_Forest/random_subspace2.png width=50% alt="Decision Tree"></p><ol start=2><li>선택된 입력 변수 중에 분할될 변수를 선택한다.</li></ol><p align=center><img src=/images/Random_Forest/random_subspace3.png width=50% alt="Decision Tree"></p><p align=center><img src=/images/Random_Forest/random_subspace4.png width=50% alt="Decision Tree"></p><ol start=3><li>이러한 과정을 <strong>full-grown tree</strong>가 될 때까지 반복한다.</li></ol><p align=center><img src=/images/Random_Forest/random_subspace5.png width=50% alt="Decision Tree"></p><p align=center><img src=/images/Random_Forest/random_subspace6.png width=50% alt="Decision Tree"></p><p>Decision Tree의 분기점을 탐색할 때, <strong>원래 변수의 수보다 적은 수의 변수를 임의로 선택</strong>하여 해당 변수들만 고려 대상으로 한다.</p><p align=center><img src=/images/Random_Forest/random_subspace7.png width=50% alt="Decision Tree"></p><p>만약 모든 트리가 같은 데이터와 같은 특성을 보고 학습한다면, 결국 비슷한 모양의 트리들만 만들어질 것이다. 이런 비슷한 트리들의 예측을 평균 낸다면 Variance 감소 효과는 미미할 것이다.</p><p>다시 정리해보자.</p><ol><li>각 트리의 노드를 분할할 때마다, 전체 feature 중 일부만 <strong>무작위</strong>로 선택한다.</li><li>선택된 feature들 중에서만 최적의 split criterion을 찾도록 강제한다.</li></ol><p>이 과정을 통해 각 트리들은 서로 다른 feature를 기반으로 성장하게 되어 모양이 더욱 <strong>다양</strong>해진다.(상관관계 감소) 이렇게 서로 다른 <strong>개성</strong>을 가진 트리들의 예측을 평균 낼 때 Variance 감소 효과는 커진다.</p><h2 id=4-generalization-error>4. Generalization Error<a hidden class=anchor aria-hidden=true href=#4-generalization-error>#</a></h2><p>각각의 개별 트리는 Overfitting될 수 있다.</p><p>Random Forest는 트리수가 충분히 많을 때 <strong>Strong Law of Large Numbers</strong>에 의해 Overfitting되지 않고 그 error는 limiting value에 수렴된다.</p><p>$$
\text{Generalization error} \leq \frac{\bar{\rho}(1 - s^2)}{s^2}
$$</p><ul><li>$\bar{\rho}$: Decision Tree 사이의 평균 상관관계</li><li>$s$: 올바로 예측한 트리와 잘못 예측한 트리 수 차이의 평균</li></ul><p>개별 트리의 정확도가 높을수록 $s$는 증가한다.</p><p>Bagging과 Random Subspace는 각 모델들의 독립성, 일반화, 무작위성을 최대화 시켜 모델간의 상관관계 $\rho$를 감소시킨다.</p><p>개별 트리의 정확도, 독립성이 높을수록 Random Forest의 성능이 높아진다.</p><h2 id=5-out-of-bag-oob>5. Out of bag (OOB)<a hidden class=anchor aria-hidden=true href=#5-out-of-bag-oob>#</a></h2><p>Decision Tree는 직관적이고 사람이 해석 가능하다. Random Forest는 Decision Tree를 수백 개를 만들어낸다. 그리고 수백 개의 트리들을 평균을 낸다.</p><p>이렇게 수백 개의 트리를 만들어내면 어떻게 이 알고리즘의 성능이 좋은지 안 좋은지를 평가할 수 있을까?</p><p>또한, 직관적이고 해석 가능한 Deicision Tree의 장점이 사라지게 되는게 아닐까?</p><p>OOB의 가장 중요한 용도는 <strong>모델 성능을 검증</strong>하는 것이다.</p><h3 id=oob-검증의-전체-과정>OOB 검증의 전체 과정<a hidden class=anchor aria-hidden=true href=#oob-검증의-전체-과정>#</a></h3><h3 id=1단계-모델-훈련>1단계. 모델 훈련<a hidden class=anchor aria-hidden=true href=#1단계-모델-훈련>#</a></h3><p>Random Forest는 수백 개의 Decision Tree로 이루어진 숲이다. 각 트리를 훈련시킬 때, 전체 훈련 데이터가 아닌 <strong>Bootstrap Sample</strong>을 사용한다.</p><ul><li>sampling: 전체 훈련 데이터(100개)에서 <strong>중복을 허용</strong>하여 <strong>무작위</strong>로 데이터 100개를 뽑는다.</li><li>OOB 샘플 발생: 이 과정에서 어떤 데이터는 여러 번 뽑히고, 어떤 데이터는 한 번도 뽑히지 않는다.<ul><li>여기서 뽑히지 않는 데이터가 나올 확률은 위에서 설명했듯이 약 0.37이다.</li></ul></li><li>예시<ul><li>트리 1의 훈련 데이터(Bootstrap sample)에는 데이터 #5가 포함되지 않았다.<ul><li>데이터 #5는 트리 1의 OOB 샘플이다.</li></ul></li><li>트리 2의 훈련 데이터에는 데이터 #5가 포함되었다.<ul><li>데이터 #5는 트리 2의 훈련 샘플(In-Bag)이다.</li></ul></li></ul></li></ul><p>이 과정을 숲의 모든 트리에 대해 반복한다. 결과적으로, 모든 데이터는 어떤 트리에게는 훈련 데이터로 쓰이고, 다른 트리에게는 OOB 샘플로 남게 된다.</p><h3 id=2단계-oob-prediction>2단계. OOB Prediction<a hidden class=anchor aria-hidden=true href=#2단계-oob-prediction>#</a></h3><p>이제 훈련이 끝났으니, 모델의 성능을 검증할 차례이다.</p><p>여기서 단 한 가지 중요한 규칙이 있다.</p><ul><li>규칙: 각 데이터는 자신을 학습하는 데 사용하지 않았던 트리들에게만 가서 예측을 받는다.</li><li>예시<ol><li>숲에 있는 500개의 트리가 있고 이 트리들 중에서 <strong>훈련할 때 데이터 #5를 사용하지 않았던 트리들</strong>(OOB 샘플로 가졌던 트리들)을 모두 찾아낸다.(트리1, 3, 6.. 499번 등 약 200개의 트리들을 찾았다고 해보자.)</li><li>이 트리들에게만 데이터 #5를 보여주고 예측을 진행한다.</li><li>이 트리들의 예측 결과를 종합한다(classification이면 다수결, regression이면 평균)</li><li>이렇게 얻은 최종 결론이 바로 데이터 #5에 대한 OOB 예측값이다.</li></ol></li></ul><p>이 과정을 훈련 데이터에서 모든 개별 데이터에 대해 반복한다.(데이터 #1부터 #100까지 각각의 OOB 예측값을 모두 구한다.)</p><h3 id=3단계-oob-score-계산>3단계. OOB Score 계산<a hidden class=anchor aria-hidden=true href=#3단계-oob-score-계산>#</a></h3><p>이제 모든 데이터에 대한 OOB 예측값과 실제 정답을 확보했다. 마지막으로 이 둘을 비교하여 최종 성능을 계산한다.</p><ul><li>데이터 #1의 OOB 예측값 &lt;-> 데이터 #1의 실제 정답</li><li>데이터 #2의 OOB 예측값 &lt;-> 데이터 #2의 실제 정답</li><li>&mldr;</li><li>데이터 #100의 OOB 예측값 &lt;-> 데이터 #100의 실제 정답</li></ul><p>이 비교 결과를 바탕으로 모델의 전체적인 성능 지표(정확도, 정밀도, MSE 등)을 계산한다. 이렇게 계산된 최종 성능 지표가 바로 <strong>OOB Score</strong>이다.</p><p>OOB Score는 모델이 처음 보는 데이터에 대해서 얼마나 잘 작동할지를 나타내는 신뢰할 수 있는 <strong>generalization</strong> 성능을 추정할 수 있다.</p><h2 id=6-featrue-importance>6. Featrue Importance<a hidden class=anchor aria-hidden=true href=#6-featrue-importance>#</a></h2><p>OOB를 통해 변수의 중요도를 구할 수 있다.</p><p>Random Forest에서 변수의 중요도가 높다면</p><ol><li>Random permutation 전-후의 OOB Error 차이가 크게 나타나야 한다.</li></ol><ul><li>Random permutation 전은 OOB data에서 어떤 변수의 관측치들을 섞기 전이고 이에 대한 OOB error $p_i$를 계산한다.</li><li>Random permutation 후는 OOB data에서 어떤 변수의 관측치들을 <strong>무작위로 섞고</strong> OOB error $e_i$를 계산한다.</li><li>차이를 구한다 $d_i^m = p_i^m - e_i^m$, (기준 성능) - (뒤섞은 후 성능)</li><li>이 차이가 클수록 그 변수가 모델의 예측에 중요하게 사용되었다는 의미다.</li></ul><ol start=2><li>그 차이의 편차가 적어야 한다.</li></ol><p>m번째 트리에서 변수 i에 대한 Random permutation 전후 OOB error의 차이는</p><p>$$d_i^m = p_i^m - e_i^m$$</p><p>전체 트리들에 대한 OOB error 차이의 평균 및 분산</p><p>$$\overline{d}<em>i = \frac{1}{m} \sum</em>{i=1}^{m} d_i^m$$</p><p>$$s_i^2 = \frac{1}{m-1} \sum_{i=1}^{m} (d_i^m - \bar{d}_i)^2$$</p><p>i번째 변수의 중요도</p><p>$$v_i = \frac{\bar{d}_i}{s_i}$$</p><p align=center><img src=/images/Random_Forest/oob2.png width=50% alt="Decision Tree"></p><p>이렇게 변수의 중요도를 측정하고 barplot으로 나타낸다면</p><p align=center><img src=/images/Random_Forest/feature_importance.png width=50% alt="Decision Tree"></p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Random Forest는 Ensemble 기법인 Bagging 사용하여 단일 Decision Tree의 높은 Variance를 줄인다.</p><p>Variance가 줄으면서 모델은 새로운 데이터에 대해서 민감하게 반응하지 않고 강건하고 generalization 성능이 높은 모델이 된다.</p><p>또한, 단일 Decision Tree는 2차원으로 생각했을 때 이진 경계면을 생성하지만 Random Forest에서 수백 개의 Decision Tree를 만들고 모든 트리에 대해서 평균을 내므로 예측값이 연속형의 스코어 가깝게 생성된다.</p><p align=center><img src=/images/Random_Forest/image.png width=50% alt="Decision Tree"></p><p>필자도 AI 대회에서 다양한 알고리즘에 대한 테스트를 수행하기 전에 Random forest를 baseline으로 먼저 적용해본다.</p><p>학습 hyperparameter에 대해서 variance가 크게 낮은 모델이기 때문에 왠만한 문제에 있어서 좋은 성능을 보이기 때문이다.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://xooyong.github.io/tags/ml/>ML</a></li></ul><nav class=paginav><a class=next href=https://xooyong.github.io/posts/2025-07/2025-07-07-decision_tree/><span class=title>Next »</span><br><span>Decision Tree</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Random Forest on x" href="https://x.com/intent/tweet/?text=Random%20Forest&amp;url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-12-random_forest%2f&amp;hashtags=ML"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Random Forest on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-12-random_forest%2f&amp;title=Random%20Forest&amp;summary=Random%20Forest&amp;source=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-12-random_forest%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Random Forest on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-12-random_forest%2f&title=Random%20Forest"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Random Forest on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-12-random_forest%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Random Forest on whatsapp" href="https://api.whatsapp.com/send?text=Random%20Forest%20-%20https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-12-random_forest%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Random Forest on telegram" href="https://telegram.me/share/url?text=Random%20Forest&amp;url=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-12-random_forest%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Random Forest on ycombinator" href="https://news.ycombinator.com/submitlink?t=Random%20Forest&u=https%3a%2f%2fxooyong.github.io%2fposts%2f2025-07%2f2025-07-12-random_forest%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://xooyong.github.io/>xooyong blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>